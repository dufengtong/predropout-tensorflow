D:\Anaconda2\envs\py3\envs\tensorflow\python.exe D:/dft/program/predropout-tensorflow/mnist_3.0_convolutional_predropout.py
Tensorflow version 1.2.0
Extracting data\train-images-idx3-ubyte.gz
Extracting data\train-labels-idx1-ubyte.gz
Extracting data\t10k-images-idx3-ubyte.gz
Extracting data\t10k-labels-idx1-ubyte.gz
2018-03-09 18:18:31.607143: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.607432: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.607708: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.607983: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.608262: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.608537: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.609455: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.609731: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:18:31.860047: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.645
pciBusID 0000:01:00.0
Total memory: 8.00GiB
Free memory: 6.65GiB
2018-03-09 18:18:31.860347: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0 
2018-03-09 18:18:31.860494: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y 
2018-03-09 18:18:31.860655: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
0: accuracy:0.12 loss: 229.883 (lr:0.003)
0: ********* epoch 1 ********* test accuracy:0.1277 test loss: 230.162
D:\Anaconda2\envs\py3\envs\tensorflow\lib\site-packages\matplotlib\backend_bases.py:2453: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented
  warnings.warn(str, mplDeprecation)
10: accuracy:0.25 loss: 222.3 (lr:0.0029855361896587787)
20: accuracy:0.47 loss: 155.647 (lr:0.0029711445178725875)
30: accuracy:0.63 loss: 88.9539 (lr:0.0029568246248488817)
40: accuracy:0.75 loss: 83.8361 (lr:0.0029425761525895904)
50: accuracy:0.78 loss: 71.6032 (lr:0.0029283987448821647)
60: accuracy:0.8 loss: 60.1451 (lr:0.0029142920472906737)
70: accuracy:0.87 loss: 76.0996 (lr:0.0029002557071469426)
80: accuracy:0.88 loss: 55.2625 (lr:0.0028862893735417373)
90: accuracy:0.89 loss: 62.3001 (lr:0.0028723926973159898)
100: accuracy:0.92 loss: 38.8572 (lr:0.0028585653310520707)
100: ********* epoch 1 ********* test accuracy:0.8747 test loss: 49.8035
110: accuracy:0.84 loss: 57.5196 (lr:0.002844806929065103)
120: accuracy:0.84 loss: 63.8151 (lr:0.0028311171473943213)
130: accuracy:0.94 loss: 35.9339 (lr:0.00281749564379447)
140: accuracy:0.91 loss: 43.4207 (lr:0.0028039420777272502)
150: accuracy:0.87 loss: 47.0722 (lr:0.0027904561103528035)
160: accuracy:0.9 loss: 32.297 (lr:0.0027770374045212438)
170: accuracy:0.86 loss: 47.5042 (lr:0.0027636856247642266)
180: accuracy:0.88 loss: 48.6153 (lr:0.0027504004372865616)
190: accuracy:0.88 loss: 46.8082 (lr:0.002737181509957871)
200: accuracy:0.88 loss: 49.5709 (lr:0.0027240285123042826)
200: ********* epoch 1 ********* test accuracy:0.9184 test loss: 35.3169
210: accuracy:0.92 loss: 38.2753 (lr:0.0027109411155001703)
220: accuracy:0.91 loss: 44.979 (lr:0.0026979189923599317)
230: accuracy:0.89 loss: 44.5533 (lr:0.002684961817329811)
240: accuracy:0.9 loss: 39.8177 (lr:0.0026720692664797567)
250: accuracy:0.95 loss: 34.4555 (lr:0.002659241017495327)
260: accuracy:0.92 loss: 30.9403 (lr:0.0026464767496696276)
270: accuracy:0.88 loss: 48.846 (lr:0.0026337761438952998)
280: accuracy:0.9 loss: 45.3908 (lr:0.002621138882656537)
290: accuracy:0.89 loss: 34.3856 (lr:0.0026085646500211496)
300: accuracy:0.91 loss: 34.7703 (lr:0.0025960531316326675)
300: ********* epoch 1 ********* test accuracy:0.9339 test loss: 28.6793
310: accuracy:0.93 loss: 25.0753 (lr:0.002583604014702479)
320: accuracy:0.92 loss: 31.0688 (lr:0.002571216988002013)
330: accuracy:0.94 loss: 29.9717 (lr:0.002558891741854956)
340: accuracy:0.93 loss: 30.3165 (lr:0.002546627968129513)
350: accuracy:0.97 loss: 15.1828 (lr:0.0025344253602307015)
360: accuracy:0.97 loss: 14.0508 (lr:0.0025222836130926888)
370: accuracy:0.92 loss: 27.0672 (lr:0.0025102024231711643)
380: accuracy:0.96 loss: 21.7144 (lr:0.0024981814884357505)
390: accuracy:0.96 loss: 17.2343 (lr:0.0024862205083624536)
400: accuracy:0.93 loss: 25.4152 (lr:0.0024743191839261473)
400: ********* epoch 1 ********* test accuracy:0.9499 test loss: 21.9807
410: accuracy:0.91 loss: 29.0098 (lr:0.002462477217593102)
420: accuracy:0.94 loss: 17.3409 (lr:0.0024506943133135424)
430: accuracy:0.94 loss: 18.8177 (lr:0.002438970176514248)
440: accuracy:0.94 loss: 16.5685 (lr:0.0024273045140911875)
450: accuracy:0.97 loss: 24.615 (lr:0.0024156970344021934)
460: accuracy:0.98 loss: 12.0992 (lr:0.0024041474472596687)
470: accuracy:0.94 loss: 24.2981 (lr:0.0023926554639233334)
480: accuracy:0.96 loss: 21.1832 (lr:0.002381220797093005)
490: accuracy:0.92 loss: 21.3649 (lr:0.0023698431609014176)
500: accuracy:0.92 loss: 21.8575 (lr:0.002358522270907074)
500: ********* epoch 1 ********* test accuracy:0.9555 test loss: 19.5787
510: accuracy:0.97 loss: 15.7747 (lr:0.002347257844087135)
520: accuracy:0.94 loss: 18.8109 (lr:0.0023360495988303423)
530: accuracy:0.98 loss: 15.9639 (lr:0.0023248972549299815)
540: accuracy:0.95 loss: 18.7834 (lr:0.002313800533576874)
550: accuracy:0.99 loss: 11.4139 (lr:0.0023027591573524086)
560: accuracy:0.97 loss: 18.1181 (lr:0.0022917728502216037)
570: accuracy:1.0 loss: 10.9133 (lr:0.0022808413375262097)
580: accuracy:0.95 loss: 20.2529 (lr:0.0022699643459778394)
590: accuracy:0.92 loss: 27.5575 (lr:0.0022591416036511374)
600: accuracy:0.96 loss: 15.2855 (lr:0.002248372839976982)
600: ********* epoch 2 ********* test accuracy:0.965 test loss: 14.9501
610: accuracy:0.97 loss: 12.5806 (lr:0.0022376577857357205)
620: accuracy:0.95 loss: 16.4771 (lr:0.0022269961730504387)
630: accuracy:0.96 loss: 20.6272 (lr:0.0022163877353802647)
640: accuracy:0.96 loss: 13.9952 (lr:0.0022058322075137037)
650: accuracy:0.96 loss: 13.756 (lr:0.0021953293255620094)
660: accuracy:0.96 loss: 13.0956 (lr:0.0021848788269525857)
670: accuracy:0.99 loss: 7.32161 (lr:0.0021744804504224237)
680: accuracy:0.98 loss: 14.9816 (lr:0.002164133936011568)
690: accuracy:0.96 loss: 22.1317 (lr:0.00215383902505662)
700: accuracy:0.97 loss: 14.1153 (lr:0.002143595460184269)
700: ********* epoch 2 ********* test accuracy:0.9658 test loss: 13.3952
710: accuracy:0.99 loss: 6.31004 (lr:0.0021334029853048602)
720: accuracy:0.95 loss: 14.8399 (lr:0.00212326134560599)
730: accuracy:0.97 loss: 11.9913 (lr:0.002113170287546139)
740: accuracy:0.99 loss: 11.9274 (lr:0.0021031295588483287)
750: accuracy:0.97 loss: 13.6205 (lr:0.0020931389084938193)
760: accuracy:0.93 loss: 16.4088 (lr:0.002083198086715832)
770: accuracy:0.99 loss: 7.26206 (lr:0.002073306844993304)
780: accuracy:0.98 loss: 5.68297 (lr:0.0020634649360446776)
790: accuracy:0.96 loss: 11.2887 (lr:0.0020536721138217163)
800: accuracy:0.94 loss: 15.9674 (lr:0.002043928133503354)
800: ********* epoch 2 ********* test accuracy:0.9692 test loss: 12.1827
810: accuracy:0.96 loss: 10.7103 (lr:0.002034232751489576)
820: accuracy:0.99 loss: 6.10902 (lr:0.0020245857253953265)
830: accuracy:0.95 loss: 14.5027 (lr:0.00201498681404445)
840: accuracy:0.98 loss: 19.6998 (lr:0.0020054357774636645)
850: accuracy:0.96 loss: 17.0693 (lr:0.001995932376876557)
860: accuracy:0.99 loss: 7.1417 (lr:0.001986476374697618)
870: accuracy:0.96 loss: 12.4828 (lr:0.0019770675345263007)
880: accuracy:0.99 loss: 6.89931 (lr:0.00196770562114111)
890: accuracy:0.96 loss: 14.4604 (lr:0.0019583904004937245)
900: accuracy:0.98 loss: 8.92358 (lr:0.001949121639703143)
900: ********* epoch 2 ********* test accuracy:0.9731 test loss: 10.642
910: accuracy:0.97 loss: 10.0355 (lr:0.001939899107049862)
920: accuracy:0.99 loss: 8.54719 (lr:0.0019307225719700854)
930: accuracy:0.98 loss: 13.9439 (lr:0.0019215918050499584)
940: accuracy:0.98 loss: 12.2754 (lr:0.0019125065780198325)
950: accuracy:0.97 loss: 10.6505 (lr:0.0019034666637485586)
960: accuracy:0.97 loss: 8.38686 (lr:0.0018944718362378086)
970: accuracy:0.96 loss: 10.6729 (lr:0.001885521870616427)
980: accuracy:0.96 loss: 20.4049 (lr:0.0018766165431348069)
990: accuracy:0.98 loss: 5.76069 (lr:0.001867755631159297)
1000: accuracy:1.0 loss: 3.09653 (lr:0.0018589389131666372)
1000: ********* epoch 2 ********* test accuracy:0.9756 test loss: 9.28751
1010: accuracy:0.98 loss: 11.056 (lr:0.0018501661687384176)
1020: accuracy:0.96 loss: 13.6268 (lr:0.0018414371785555712)
1030: accuracy:0.98 loss: 8.65952 (lr:0.0018327517243928889)
1040: accuracy:0.98 loss: 11.9639 (lr:0.001824109589113564)
1050: accuracy:0.97 loss: 11.0137 (lr:0.001815510556663764)
1060: accuracy:0.98 loss: 7.81198 (lr:0.0018069544120672303)
1070: accuracy:0.95 loss: 15.2084 (lr:0.001798440941419902)
1080: accuracy:0.93 loss: 28.4292 (lr:0.0017899699318845701)
1090: accuracy:0.98 loss: 6.65135 (lr:0.0017815411716855546)
1100: accuracy:0.96 loss: 12.0491 (lr:0.0017731544501034114)
1100: ********* epoch 2 ********* test accuracy:0.9758 test loss: 9.49521
1110: accuracy:0.99 loss: 6.91051 (lr:0.0017648095574696644)
1120: accuracy:0.98 loss: 7.61637 (lr:0.0017565062851615633)
1130: accuracy:0.99 loss: 6.03816 (lr:0.0017482444255968678)
1140: accuracy:0.95 loss: 13.0595 (lr:0.0017400237722286578)
1150: accuracy:0.96 loss: 9.33364 (lr:0.0017318441195401718)
1160: accuracy:0.98 loss: 9.20578 (lr:0.001723705263039666)
1170: accuracy:0.97 loss: 10.3506 (lr:0.0017156069992553045)
1180: accuracy:0.98 loss: 6.75993 (lr:0.0017075491257300707)
1190: accuracy:0.99 loss: 9.77655 (lr:0.0016995314410167067)
1200: accuracy:0.98 loss: 8.86703 (lr:0.001691553744672677)
1200: ********* epoch 3 ********* test accuracy:0.9753 test loss: 9.39526
1210: accuracy:0.97 loss: 11.0451 (lr:0.0016836158372551574)
1220: accuracy:0.96 loss: 15.3951 (lr:0.0016757175203160495)
1230: accuracy:0.99 loss: 9.05935 (lr:0.0016678585963970183)
1240: accuracy:0.98 loss: 13.7105 (lr:0.001660038869024556)
1250: accuracy:0.97 loss: 11.3872 (lr:0.001652258142705072)
1260: accuracy:0.97 loss: 9.82723 (lr:0.001644516222920002)
1270: accuracy:0.99 loss: 5.63426 (lr:0.001636812916120949)
1280: accuracy:1.0 loss: 3.28948 (lr:0.001629148029724841)
1290: accuracy:0.98 loss: 5.52367 (lr:0.0016215213721091197)
1300: accuracy:0.97 loss: 11.9581 (lr:0.0016139327526069466)
1300: ********* epoch 3 ********* test accuracy:0.9782 test loss: 8.45524
1310: accuracy:0.97 loss: 12.8599 (lr:0.00160638198150244)
1320: accuracy:0.97 loss: 10.5584 (lr:0.0015988688700259279)
1330: accuracy:0.98 loss: 6.0214 (lr:0.0015913932303492327)
1340: accuracy:0.99 loss: 7.21216 (lr:0.0015839548755809732)
1350: accuracy:0.99 loss: 7.44597 (lr:0.0015765536197618927)
1360: accuracy:0.99 loss: 7.95381 (lr:0.0015691892778602098)
1370: accuracy:1.0 loss: 3.78199 (lr:0.0015618616657669942)
1380: accuracy:0.98 loss: 6.80223 (lr:0.0015545706002915614)
1390: accuracy:0.99 loss: 6.16691 (lr:0.0015473158991568946)
1400: accuracy:0.98 loss: 8.28678 (lr:0.0015400973809950877)
1400: ********* epoch 3 ********* test accuracy:0.9756 test loss: 8.87135
1410: accuracy:0.99 loss: 4.99283 (lr:0.001532914865342811)
1420: accuracy:0.98 loss: 7.51906 (lr:0.001525768172636799)
1430: accuracy:0.99 loss: 5.36953 (lr:0.0015186571242093616)
1440: accuracy:1.0 loss: 3.1319 (lr:0.001511581542283918)
1450: accuracy:0.97 loss: 8.72667 (lr:0.0015045412499705513)
1460: accuracy:1.0 loss: 2.19583 (lr:0.0014975360712615872)
1470: accuracy:0.95 loss: 12.9072 (lr:0.0014905658310271931)
1480: accuracy:0.98 loss: 4.63608 (lr:0.0014836303550109999)
1490: accuracy:0.99 loss: 5.36559 (lr:0.0014767294698257462)
1500: accuracy:0.99 loss: 4.50143 (lr:0.0014698630029489428)
1500: ********* epoch 3 ********* test accuracy:0.9782 test loss: 7.94158
1510: accuracy:1.0 loss: 3.31706 (lr:0.0014630307827185603)
1520: accuracy:1.0 loss: 3.23551 (lr:0.0014562326383287369)
1530: accuracy:1.0 loss: 4.84304 (lr:0.001449468399825509)
1540: accuracy:0.98 loss: 11.8914 (lr:0.0014427378981025616)
1550: accuracy:0.98 loss: 13.6194 (lr:0.001436040964897001)
1560: accuracy:0.97 loss: 10.0628 (lr:0.0014293774327851483)
1570: accuracy:0.99 loss: 5.64269 (lr:0.0014227471351783538)
1580: accuracy:1.0 loss: 2.3149 (lr:0.001416149906318832)
1590: accuracy:0.98 loss: 6.90361 (lr:0.0014095855812755176)
1600: accuracy:0.98 loss: 6.16583 (lr:0.0014030539959399427)
1600: ********* epoch 3 ********* test accuracy:0.9808 test loss: 7.38207
1610: accuracy:0.97 loss: 4.78105 (lr:0.0013965549870221337)
1620: accuracy:0.98 loss: 6.65115 (lr:0.0013900883920465294)
1630: accuracy:0.98 loss: 13.8489 (lr:0.0013836540493479186)
1640: accuracy:1.0 loss: 4.05659 (lr:0.001377251798067398)
1650: accuracy:0.99 loss: 4.13883 (lr:0.001370881478148353)
1660: accuracy:0.99 loss: 4.44057 (lr:0.0013645429303324535)
1670: accuracy:0.99 loss: 3.91455 (lr:0.0013582359961556737)
1680: accuracy:1.0 loss: 3.12435 (lr:0.0013519605179443314)
1690: accuracy:0.98 loss: 4.46203 (lr:0.0013457163388111437)
1700: accuracy:0.96 loss: 15.4575 (lr:0.0013395033026513076)
1700: ********* epoch 3 ********* test accuracy:0.9801 test loss: 6.82681
1710: accuracy:0.97 loss: 9.13882 (lr:0.001333321254138595)
1720: accuracy:0.97 loss: 9.98743 (lr:0.0013271700387214717)
1730: accuracy:0.99 loss: 6.18049 (lr:0.0013210495026192315)
1740: accuracy:0.97 loss: 14.9345 (lr:0.0013149594928181531)
1750: accuracy:1.0 loss: 2.17829 (lr:0.0013088998570676745)
1760: accuracy:0.98 loss: 5.89478 (lr:0.0013028704438765861)
1770: accuracy:0.97 loss: 10.2811 (lr:0.0012968711025092442)
1780: accuracy:0.97 loss: 8.88846 (lr:0.001290901682981802)
1790: accuracy:0.98 loss: 7.65652 (lr:0.0012849620360584606)
1800: accuracy:0.99 loss: 3.46037 (lr:0.0012790520132477375)
1800: ********* epoch 4 ********* test accuracy:0.9796 test loss: 7.01368
1810: accuracy:0.98 loss: 9.18537 (lr:0.0012731714667987546)
1820: accuracy:0.99 loss: 4.98045 (lr:0.0012673202496975445)
1830: accuracy:0.98 loss: 4.21491 (lr:0.0012614982156633745)
1840: accuracy:1.0 loss: 3.17062 (lr:0.0012557052191450912)
1850: accuracy:0.98 loss: 5.698 (lr:0.0012499411153174794)
1860: accuracy:0.98 loss: 5.00894 (lr:0.0012442057600776434)
1870: accuracy:1.0 loss: 2.63939 (lr:0.0012384990100414034)
1880: accuracy:0.99 loss: 4.89701 (lr:0.0012328207225397114)
1890: accuracy:0.97 loss: 11.1655 (lr:0.001227170755615084)
1900: accuracy:1.0 loss: 2.93894 (lr:0.0012215489680180538)
1900: ********* epoch 4 ********* test accuracy:0.9786 test loss: 7.39397
1910: accuracy:1.0 loss: 3.43165 (lr:0.001215955219203638)
1920: accuracy:0.98 loss: 4.47 (lr:0.0012103893693278251)
1930: accuracy:0.98 loss: 6.78306 (lr:0.0012048512792440782)
1940: accuracy:0.98 loss: 4.20368 (lr:0.0011993408104998568)
1950: accuracy:0.99 loss: 6.22036 (lr:0.0011938578253331553)
1960: accuracy:1.0 loss: 2.04459 (lr:0.001188402186669059)
1970: accuracy:0.98 loss: 11.3003 (lr:0.0011829737581163168)
1980: accuracy:0.99 loss: 3.372 (lr:0.0011775724039639326)
1990: accuracy:0.97 loss: 8.21713 (lr:0.0011721979891777712)
2000: accuracy:0.99 loss: 4.17143 (lr:0.0011668503793971828)
2000: ********* epoch 4 ********* test accuracy:0.9822 test loss: 6.40582
2010: accuracy:0.96 loss: 28.1994 (lr:0.0011615294409316448)
2020: accuracy:0.98 loss: 8.83294 (lr:0.0011562350407574179)
2030: accuracy:0.98 loss: 6.82261 (lr:0.0011509670465142223)
2040: accuracy:0.95 loss: 11.0367 (lr:0.0011457253265019273)
2050: accuracy:0.97 loss: 12.6534 (lr:0.0011405097496772598)
2060: accuracy:0.99 loss: 5.40629 (lr:0.0011353201856505275)
2070: accuracy:0.98 loss: 6.65402 (lr:0.0011301565046823595)
2080: accuracy:0.98 loss: 5.41657 (lr:0.0011250185776804627)
2090: accuracy:0.98 loss: 5.7341 (lr:0.0011199062761963943)
2100: accuracy:1.0 loss: 2.86983 (lr:0.0011148194724223506)
2100: ********* epoch 4 ********* test accuracy:0.9812 test loss: 6.68528
2110: accuracy:0.97 loss: 9.47651 (lr:0.0011097580391879731)
2120: accuracy:1.0 loss: 2.98886 (lr:0.0011047218499571666)
2130: accuracy:0.98 loss: 9.56308 (lr:0.0010997107788249386)
2140: accuracy:0.98 loss: 5.25632 (lr:0.0010947247005142493)
2150: accuracy:1.0 loss: 2.2376 (lr:0.001089763490372882)
2160: accuracy:0.98 loss: 8.14505 (lr:0.0010848270243703235)
2170: accuracy:0.98 loss: 3.89634 (lr:0.001079915179094668)
2180: accuracy:0.99 loss: 3.28316 (lr:0.0010750278317495268)
2190: accuracy:0.98 loss: 8.33829 (lr:0.0010701648601509621)
2200: accuracy:0.98 loss: 3.2846 (lr:0.0010653261427244307)
2200: ********* epoch 4 ********* test accuracy:0.983 test loss: 6.13267
2210: accuracy:0.99 loss: 2.88287 (lr:0.001060511558501745)
2220: accuracy:0.97 loss: 5.88413 (lr:0.0010557209871180483)
2230: accuracy:0.99 loss: 3.48055 (lr:0.0010509543088088069)
2240: accuracy:0.98 loss: 6.58391 (lr:0.0010462114044068145)
2250: accuracy:0.99 loss: 3.45305 (lr:0.0010414921553392143)
2260: accuracy:0.99 loss: 4.86971 (lr:0.0010367964436245336)
2270: accuracy:0.99 loss: 2.99178 (lr:0.001032124151869735)
2280: accuracy:0.99 loss: 8.03449 (lr:0.0010274751632672816)
2290: accuracy:0.98 loss: 11.1225 (lr:0.0010228493615922153)
2300: accuracy:0.99 loss: 5.18579 (lr:0.0010182466311992545)
2300: ********* epoch 4 ********* test accuracy:0.9825 test loss: 6.14241
2310: accuracy:0.98 loss: 9.38377 (lr:0.0010136668570198987)
2320: accuracy:0.99 loss: 3.97687 (lr:0.0010091099245595554)
2330: accuracy:1.0 loss: 5.15405 (lr:0.0010045757198946755)
2340: accuracy:0.99 loss: 3.93069 (lr:0.0010000641296699067)
2350: accuracy:0.99 loss: 3.57825 (lr:0.0009955750410952577)
2360: accuracy:0.98 loss: 10.121 (lr:0.0009911083419432806)
2370: accuracy:0.98 loss: 7.0278 (lr:0.000986663920546264)
2380: accuracy:0.98 loss: 4.64134 (lr:0.0009822416657934419)
2390: accuracy:0.99 loss: 3.64857 (lr:0.0009778414671282143)
2400: accuracy:0.97 loss: 7.2529 (lr:0.0009734632145453863)
2400: ********* epoch 5 ********* test accuracy:0.9838 test loss: 5.79808
2410: accuracy:1.0 loss: 1.46813 (lr:0.0009691067985884144)
2420: accuracy:0.97 loss: 8.31631 (lr:0.0009647721103466736)
2430: accuracy:0.97 loss: 7.44388 (lr:0.0009604590414527313)
2440: accuracy:0.99 loss: 4.74239 (lr:0.0009561674840796413)
2450: accuracy:1.0 loss: 1.76936 (lr:0.0009518973309382452)
2460: accuracy:0.99 loss: 2.95127 (lr:0.0009476484752744924)
2470: accuracy:0.98 loss: 5.68223 (lr:0.0009434208108667696)
2480: accuracy:1.0 loss: 3.40351 (lr:0.0009392142320232469)
2490: accuracy:0.99 loss: 2.25725 (lr:0.0009350286335792337)
2500: accuracy:0.99 loss: 2.92902 (lr:0.0009308639108945514)
2500: ********* epoch 5 ********* test accuracy:0.9807 test loss: 6.43152
2510: accuracy:0.99 loss: 5.67999 (lr:0.0009267199598509155)
2520: accuracy:1.0 loss: 2.17835 (lr:0.0009225966768493343)
2530: accuracy:0.99 loss: 3.12815 (lr:0.0009184939588075178)
2540: accuracy:0.97 loss: 9.81133 (lr:0.0009144117031573014)
2550: accuracy:0.99 loss: 5.25595 (lr:0.0009103498078420814)
2560: accuracy:0.97 loss: 6.41113 (lr:0.0009063081713142631)
2570: accuracy:0.98 loss: 5.58834 (lr:0.0009022866925327231)
2580: accuracy:1.0 loss: 1.33485 (lr:0.0008982852709602818)
2590: accuracy:0.98 loss: 7.139 (lr:0.0008943038065611923)
2600: accuracy:1.0 loss: 3.39666 (lr:0.0008903421997986366)
2600: ********* epoch 5 ********* test accuracy:0.982 test loss: 5.89605
2610: accuracy:0.99 loss: 3.94751 (lr:0.0008864003516322398)
2620: accuracy:1.0 loss: 2.33724 (lr:0.0008824781635155918)
2630: accuracy:0.98 loss: 4.45426 (lr:0.0008785755373937862)
2640: accuracy:0.99 loss: 3.72486 (lr:0.000874692375700966)
2650: accuracy:1.0 loss: 2.56434 (lr:0.0008708285813578872)
2660: accuracy:1.0 loss: 1.17251 (lr:0.0008669840577694896)
2670: accuracy:1.0 loss: 1.21535 (lr:0.0008631587088224834)
2680: accuracy:1.0 loss: 2.17472 (lr:0.0008593524388829455)
2690: accuracy:1.0 loss: 3.91481 (lr:0.0008555651527939294)
2700: accuracy:1.0 loss: 2.72087 (lr:0.0008517967558730855)
2700: ********* epoch 5 ********* test accuracy:0.9834 test loss: 5.88411
2710: accuracy:1.0 loss: 2.20966 (lr:0.0008480471539102946)
2720: accuracy:0.98 loss: 5.47796 (lr:0.0008443162531653122)
2730: accuracy:0.97 loss: 10.2834 (lr:0.0008406039603654254)
2740: accuracy:0.99 loss: 4.70044 (lr:0.0008369101827031209)
2750: accuracy:0.99 loss: 3.06585 (lr:0.0008332348278337648)
2760: accuracy:0.98 loss: 4.89645 (lr:0.000829577803873294)
2770: accuracy:0.98 loss: 5.61265 (lr:0.0008259390193959188)
2780: accuracy:1.0 loss: 2.40637 (lr:0.000822318383431838)
2790: accuracy:0.99 loss: 3.65205 (lr:0.0008187158054649635)
2800: accuracy:1.0 loss: 1.09114 (lr:0.0008151311954306589)
2800: ********* epoch 5 ********* test accuracy:0.9832 test loss: 5.60982
2810: accuracy:0.97 loss: 6.67531 (lr:0.0008115644637134865)
2820: accuracy:1.0 loss: 2.15251 (lr:0.0008080155211449677)
2830: accuracy:1.0 loss: 1.15718 (lr:0.0008044842790013532)
2840: accuracy:0.99 loss: 2.91461 (lr:0.0008009706490014058)
2850: accuracy:0.98 loss: 11.3044 (lr:0.0007974745433041923)
2860: accuracy:1.0 loss: 1.23743 (lr:0.0007939958745068883)
2870: accuracy:0.98 loss: 13.2981 (lr:0.0007905345556425924)
2880: accuracy:1.0 loss: 3.58206 (lr:0.0007870905001781532)
2890: accuracy:0.97 loss: 16.8562 (lr:0.0007836636220120043)
2900: accuracy:1.0 loss: 1.66769 (lr:0.0007802538354720133)
2900: ********* epoch 5 ********* test accuracy:0.9851 test loss: 5.17669
2910: accuracy:0.98 loss: 14.7226 (lr:0.000776861055313339)
2920: accuracy:0.95 loss: 13.0341 (lr:0.0007734851967163007)
2930: accuracy:1.0 loss: 1.76319 (lr:0.0007701261752842576)
2940: accuracy:0.99 loss: 2.55373 (lr:0.0007667839070414992)
2950: accuracy:0.99 loss: 4.18297 (lr:0.0007634583084311452)
2960: accuracy:0.98 loss: 4.95535 (lr:0.000760149296313057)
2970: accuracy:1.0 loss: 1.504 (lr:0.0007568567879617594)
2980: accuracy:0.98 loss: 4.05451 (lr:0.0007535807010643724)
2990: accuracy:0.98 loss: 24.9863 (lr:0.0007503209537185525)
3000: accuracy:0.99 loss: 3.65196 (lr:0.0007470774644304465)
3000: ********* epoch 6 ********* test accuracy:0.9858 test loss: 5.24764
3010: accuracy:0.98 loss: 7.5892 (lr:0.0007438501521126534)
3020: accuracy:0.99 loss: 9.68393 (lr:0.0007406389360821968)
3030: accuracy:1.0 loss: 1.45761 (lr:0.0007374437360585091)
3040: accuracy:0.97 loss: 6.23481 (lr:0.0007342644721614229)
3050: accuracy:0.97 loss: 5.89719 (lr:0.0007311010649091755)
3060: accuracy:1.0 loss: 2.58576 (lr:0.0007279534352164206)
3070: accuracy:0.97 loss: 4.99506 (lr:0.0007248215043922522)
3080: accuracy:1.0 loss: 1.56513 (lr:0.0007217051941382361)
3090: accuracy:0.97 loss: 4.29051 (lr:0.0007186044265464543)
3100: accuracy:0.99 loss: 2.05582 (lr:0.0007155191240975549)
3100: ********* epoch 6 ********* test accuracy:0.983 test loss: 5.6496
3110: accuracy:0.98 loss: 15.7123 (lr:0.0007124492096588165)
3120: accuracy:0.98 loss: 4.90687 (lr:0.0007093946064822178)
3130: accuracy:0.97 loss: 5.18924 (lr:0.0007063552382025206)
3140: accuracy:1.0 loss: 2.83706 (lr:0.0007033310288353594)
3150: accuracy:0.98 loss: 3.65985 (lr:0.0007003219027753427)
3160: accuracy:1.0 loss: 2.26594 (lr:0.000697327784794162)
3170: accuracy:1.0 loss: 1.36481 (lr:0.0006943486000387123)
3180: accuracy:1.0 loss: 1.98675 (lr:0.000691384274029219)
3190: accuracy:0.98 loss: 6.95537 (lr:0.0006884347326573779)
3200: accuracy:0.99 loss: 2.7973 (lr:0.0006854999021845007)
3200: ********* epoch 6 ********* test accuracy:0.9838 test loss: 5.2994
3210: accuracy:1.0 loss: 1.02921 (lr:0.0006825797092396732)
3220: accuracy:0.98 loss: 7.42245 (lr:0.0006796740808179191)
3230: accuracy:0.96 loss: 5.24297 (lr:0.000676782944278377)
3240: accuracy:1.0 loss: 1.15354 (lr:0.0006739062273424826)
3250: accuracy:1.0 loss: 1.71037 (lr:0.0006710438580921628)
3260: accuracy:0.99 loss: 3.16927 (lr:0.0006681957649680373)
3270: accuracy:1.0 loss: 1.10474 (lr:0.0006653618767676294)
3280: accuracy:1.0 loss: 2.0101 (lr:0.0006625421226435866)
3290: accuracy:1.0 loss: 1.63023 (lr:0.0006597364321019091)
3300: accuracy:0.99 loss: 3.55599 (lr:0.000656944735000187)
3300: ********* epoch 6 ********* test accuracy:0.9845 test loss: 5.43091
3310: accuracy:1.0 loss: 1.5211 (lr:0.0006541669615458476)
3320: accuracy:0.99 loss: 4.7382 (lr:0.0006514030422944097)
3330: accuracy:1.0 loss: 1.87343 (lr:0.000648652908147748)
3340: accuracy:0.99 loss: 3.17989 (lr:0.0006459164903523658)
3350: accuracy:0.98 loss: 4.63115 (lr:0.0006431937204976754)
3360: accuracy:1.0 loss: 0.770546 (lr:0.000640484530514289)
3370: accuracy:0.99 loss: 2.35238 (lr:0.0006377888526723156)
3380: accuracy:0.98 loss: 5.61475 (lr:0.000635106619579669)
3390: accuracy:1.0 loss: 2.2097 (lr:0.0006324377641803819)
3400: accuracy:0.96 loss: 12.6005 (lr:0.0006297822197529307)
3400: ********* epoch 6 ********* test accuracy:0.984 test loss: 5.33026
3410: accuracy:0.99 loss: 4.22769 (lr:0.0006271399199085659)
3420: accuracy:1.0 loss: 0.655592 (lr:0.0006245107985896541)
3430: accuracy:0.98 loss: 3.52009 (lr:0.0006218947900680254)
3440: accuracy:0.99 loss: 3.32227 (lr:0.0006192918289433304)
3450: accuracy:0.99 loss: 2.73838 (lr:0.0006167018501414055)
3460: accuracy:0.98 loss: 4.41555 (lr:0.0006141247889126458)
3470: accuracy:1.0 loss: 1.17585 (lr:0.0006115605808303861)
3480: accuracy:0.98 loss: 7.94448 (lr:0.000609009161789291)
3490: accuracy:0.99 loss: 2.46269 (lr:0.0006064704680037515)
3500: accuracy:0.98 loss: 5.42958 (lr:0.000603944436006291)
3500: ********* epoch 6 ********* test accuracy:0.9838 test loss: 5.26503
3510: accuracy:1.0 loss: 1.99932 (lr:0.0006014310026459777)
3520: accuracy:1.0 loss: 1.48115 (lr:0.0005989301050868467)
3530: accuracy:1.0 loss: 1.75163 (lr:0.0005964416808063288)
3540: accuracy:0.99 loss: 3.72195 (lr:0.0005939656675936874)
3550: accuracy:0.99 loss: 2.01461 (lr:0.0005915020035484634)
3560: accuracy:0.98 loss: 4.44796 (lr:0.000589050627078927)
3570: accuracy:0.99 loss: 2.40908 (lr:0.0005866114769005391)
3580: accuracy:1.0 loss: 1.80158 (lr:0.000584184492034418)
3590: accuracy:0.99 loss: 2.64481 (lr:0.000581769611805816)
3600: accuracy:1.0 loss: 2.60525 (lr:0.000579366775842601)
3600: ********* epoch 7 ********* test accuracy:0.9861 test loss: 4.96116
3610: accuracy:1.0 loss: 1.39206 (lr:0.0005769759240737492)
3620: accuracy:0.99 loss: 4.84156 (lr:0.0005745969967278418)
3630: accuracy:0.98 loss: 3.95484 (lr:0.0005722299343315713)
3640: accuracy:0.98 loss: 2.83056 (lr:0.0005698746777082543)
3650: accuracy:1.0 loss: 0.864386 (lr:0.0005675311679763527)
3660: accuracy:0.99 loss: 2.90493 (lr:0.000565199346548001)
3670: accuracy:0.98 loss: 3.38441 (lr:0.0005628791551275424)
3680: accuracy:0.99 loss: 2.92518 (lr:0.00056057053571007)
3690: accuracy:0.99 loss: 1.29546 (lr:0.0005582734305799787)
3700: accuracy:1.0 loss: 1.2181 (lr:0.0005559877823095201)
3700: ********* epoch 7 ********* test accuracy:0.9846 test loss: 5.07029
3710: accuracy:0.96 loss: 10.81 (lr:0.0005537135337573688)
3720: accuracy:0.99 loss: 2.89617 (lr:0.0005514506280671922)
3730: accuracy:1.0 loss: 1.16645 (lr:0.0005491990086662305)
3740: accuracy:0.99 loss: 4.27619 (lr:0.0005469586192638811)
3750: accuracy:0.98 loss: 5.79746 (lr:0.0005447294038502926)
3760: accuracy:1.0 loss: 1.09696 (lr:0.0005425113066949633)
3770: accuracy:1.0 loss: 1.51156 (lr:0.0005403042723453488)
3780: accuracy:1.0 loss: 1.0916 (lr:0.0005381082456254755)
3790: accuracy:0.99 loss: 4.63551 (lr:0.0005359231716345609)
3800: accuracy:1.0 loss: 1.43578 (lr:0.0005337489957456418)
3800: ********* epoch 7 ********* test accuracy:0.9852 test loss: 5.10783
3810: accuracy:0.99 loss: 2.37964 (lr:0.0005315856636042072)
3820: accuracy:0.99 loss: 5.44898 (lr:0.0005294331211268412)
3830: accuracy:0.98 loss: 4.04511 (lr:0.0005272913144998697)
3840: accuracy:0.99 loss: 17.9149 (lr:0.0005251601901780155)
3850: accuracy:1.0 loss: 2.50228 (lr:0.0005230396948830594)
3860: accuracy:0.99 loss: 2.85571 (lr:0.0005209297756025089)
3870: accuracy:0.98 loss: 7.50172 (lr:0.0005188303795882718)
3880: accuracy:0.99 loss: 1.90598 (lr:0.0005167414543553385)
3890: accuracy:0.98 loss: 8.33751 (lr:0.0005146629476804694)
3900: accuracy:1.0 loss: 1.60804 (lr:0.0005125948076008895)
3900: ********* epoch 7 ********* test accuracy:0.9842 test loss: 5.115
3910: accuracy:0.99 loss: 3.69825 (lr:0.0005105369824129888)
3920: accuracy:0.97 loss: 8.48261 (lr:0.0005084894206710306)
3930: accuracy:1.0 loss: 2.02469 (lr:0.0005064520711858646)
3940: accuracy:0.99 loss: 1.68 (lr:0.0005044248830236478)
3950: accuracy:1.0 loss: 1.41236 (lr:0.0005024078055045702)
3960: accuracy:0.99 loss: 11.6702 (lr:0.0005004007882015892)
3970: accuracy:0.99 loss: 3.10058 (lr:0.0004984037809391673)
3980: accuracy:0.98 loss: 6.19379 (lr:0.0004964167337920192)
3990: accuracy:1.0 loss: 1.35791 (lr:0.0004944395970838627)
4000: accuracy:0.97 loss: 9.4492 (lr:0.0004924723213861769)
4000: ********* epoch 7 ********* test accuracy:0.9848 test loss: 4.82651
4010: accuracy:1.0 loss: 2.62539 (lr:0.000490514857516967)
4020: accuracy:1.0 loss: 1.8675 (lr:0.0004885671565395345)
4030: accuracy:0.99 loss: 2.42198 (lr:0.00048662916976125316)
4040: accuracy:1.0 loss: 1.02936 (lr:0.00048470084873235297)
4050: accuracy:1.0 loss: 1.11627 (lr:0.00048278214524470766)
4060: accuracy:0.99 loss: 5.42309 (lr:0.00048087301133063005)
4070: accuracy:0.98 loss: 3.47729 (lr:0.0004789733992616726)
4080: accuracy:0.99 loss: 5.52043 (lr:0.00047708326154743513)
4090: accuracy:0.97 loss: 4.81501 (lr:0.00047520255093437617)
4100: accuracy:1.0 loss: 0.700769 (lr:0.0004733312204046323)
4100: ********* epoch 7 ********* test accuracy:0.9856 test loss: 4.83314
4110: accuracy:0.98 loss: 5.27651 (lr:0.0004714692231748428)
4120: accuracy:0.98 loss: 5.01522 (lr:0.0004696165126949802)
4130: accuracy:0.97 loss: 7.65169 (lr:0.0004677730426471858)
4140: accuracy:0.98 loss: 3.52333 (lr:0.00046593876694461247)
4150: accuracy:0.99 loss: 2.29037 (lr:0.0004641136397302719)
4160: accuracy:0.98 loss: 6.49075 (lr:0.000462297615375889)
4170: accuracy:1.0 loss: 1.68533 (lr:0.0004604906484807602)
4180: accuracy:1.0 loss: 0.715734 (lr:0.000458692693870619)
4190: accuracy:0.98 loss: 5.65406 (lr:0.0004569037065965064)
4200: accuracy:0.99 loss: 1.77173 (lr:0.00045512364193364755)
4200: ********* epoch 8 ********* test accuracy:0.9858 test loss: 4.66905
4210: accuracy:1.0 loss: 1.03616 (lr:0.00045335245538033307)
4220: accuracy:1.0 loss: 1.07702 (lr:0.0004515901026568069)
4230: accuracy:0.99 loss: 2.99288 (lr:0.000449836539704159)
4240: accuracy:1.0 loss: 0.91168 (lr:0.00044809172268322453)
4250: accuracy:1.0 loss: 0.561445 (lr:0.00044635560797348693)
4260: accuracy:0.97 loss: 4.12672 (lr:0.00044462815217198804)
4270: accuracy:1.0 loss: 0.959699 (lr:0.0004429093120922428)
4280: accuracy:0.99 loss: 1.66279 (lr:0.0004411990447631597)
4290: accuracy:1.0 loss: 1.76334 (lr:0.0004394973074279665)
4300: accuracy:1.0 loss: 0.810016 (lr:0.00043780405754314123)
4300: ********* epoch 8 ********* test accuracy:0.9853 test loss: 4.83378
4310: accuracy:1.0 loss: 2.2783 (lr:0.00043611925277734854)
4320: accuracy:0.99 loss: 2.54726 (lr:0.0004344428510103813)
4330: accuracy:0.97 loss: 5.57777 (lr:0.0004327748103321084)
4340: accuracy:0.99 loss: 2.45541 (lr:0.00043111508904142585)
4350: accuracy:1.0 loss: 0.857428 (lr:0.00042946364564521496)
4360: accuracy:1.0 loss: 1.99391 (lr:0.0004278204388573046)
4370: accuracy:0.99 loss: 1.53674 (lr:0.0004261854275974398)
4380: accuracy:0.98 loss: 7.6338 (lr:0.0004245585709902538)
4390: accuracy:1.0 loss: 0.915144 (lr:0.0004229398283642466)
4400: accuracy:1.0 loss: 1.99215 (lr:0.00042132915925076824)
4400: ********* epoch 8 ********* test accuracy:0.9848 test loss: 4.88666
4410: accuracy:0.99 loss: 1.43493 (lr:0.00041972652338300717)
4420: accuracy:1.0 loss: 1.29601 (lr:0.0004181318806949831)
4430: accuracy:0.99 loss: 6.205 (lr:0.0004165451913205458)
4440: accuracy:1.0 loss: 0.762896 (lr:0.0004149664155923781)
4450: accuracy:0.99 loss: 3.18704 (lr:0.00041339551404100485)
4460: accuracy:0.99 loss: 2.01595 (lr:0.0004118324473938054)
4470: accuracy:0.99 loss: 3.59537 (lr:0.00041027717657403205)
4480: accuracy:0.98 loss: 7.20152 (lr:0.00040872966269983314)
4490: accuracy:1.0 loss: 0.871726 (lr:0.00040718986708328155)
4500: accuracy:1.0 loss: 1.41396 (lr:0.00040565775122940656)
4500: ********* epoch 8 ********* test accuracy:0.9863 test loss: 4.64451
4510: accuracy:0.99 loss: 4.40594 (lr:0.000404133276835232)
4520: accuracy:0.98 loss: 5.09903 (lr:0.0004026164057888186)
4530: accuracy:1.0 loss: 0.744876 (lr:0.0004011071001683111)
4540: accuracy:0.98 loss: 6.14562 (lr:0.0003996053222409906)
4550: accuracy:1.0 loss: 1.94089 (lr:0.00039811103446233054)
4560: accuracy:0.98 loss: 5.56895 (lr:0.0003966241994750587)
4570: accuracy:0.98 loss: 3.53633 (lr:0.0003951447801082228)
4580: accuracy:1.0 loss: 2.20519 (lr:0.00039367273937626184)
4590: accuracy:1.0 loss: 1.37651 (lr:0.00039220804047808086)
4600: accuracy:1.0 loss: 1.49042 (lr:0.0003907506467961309)
4600: ********* epoch 8 ********* test accuracy:0.9851 test loss: 5.04022
4610: accuracy:0.98 loss: 3.93704 (lr:0.00038930052189549403)
4620: accuracy:0.99 loss: 1.93336 (lr:0.0003878576295229724)
4630: accuracy:1.0 loss: 0.984814 (lr:0.0003864219336061815)
4640: accuracy:0.98 loss: 3.87324 (lr:0.0003849933982526485)
4650: accuracy:0.97 loss: 4.05504 (lr:0.00038357198774891513)
4660: accuracy:1.0 loss: 1.12235 (lr:0.00038215766655964504)
4670: accuracy:0.97 loss: 8.5978 (lr:0.0003807503993267346)
4680: accuracy:0.98 loss: 4.14664 (lr:0.0003793501508684298)
4690: accuracy:0.99 loss: 4.40989 (lr:0.0003779568861784461)
4700: accuracy:1.0 loss: 2.11704 (lr:0.0003765705704250939)
4700: ********* epoch 8 ********* test accuracy:0.9856 test loss: 4.83805
4710: accuracy:0.99 loss: 1.65381 (lr:0.00037519116895040703)
4720: accuracy:1.0 loss: 1.09746 (lr:0.0003738186472692768)
4730: accuracy:0.99 loss: 3.50311 (lr:0.00037245297106858964)
4740: accuracy:1.0 loss: 0.827476 (lr:0.0003710941062063696)
4750: accuracy:0.97 loss: 8.67148 (lr:0.00036974201871092414)
4760: accuracy:1.0 loss: 1.10551 (lr:0.00036839667477999555)
4770: accuracy:0.99 loss: 11.7051 (lr:0.0003670580407799155)
4780: accuracy:1.0 loss: 4.01407 (lr:0.00036572608324476403)
4790: accuracy:1.0 loss: 0.63575 (lr:0.0003644007688755338)
4800: accuracy:1.0 loss: 0.59359 (lr:0.0003630820645392963)
4800: ********* epoch 9 ********* test accuracy:0.9861 test loss: 4.74652
4810: accuracy:1.0 loss: 1.58821 (lr:0.0003617699372683745)
4820: accuracy:0.99 loss: 2.12275 (lr:0.00036046435425951816)
4830: accuracy:1.0 loss: 1.95486 (lr:0.00035916528287308427)
4840: accuracy:0.99 loss: 2.61371 (lr:0.00035787269063222043)
4850: accuracy:1.0 loss: 0.840743 (lr:0.0003565865452220532)
4860: accuracy:1.0 loss: 0.683695 (lr:0.0003553068144888804)
4870: accuracy:1.0 loss: 1.14829 (lr:0.00035403346643936713)
4880: accuracy:1.0 loss: 0.828996 (lr:0.00035276646923974576)
4890: accuracy:0.99 loss: 4.34552 (lr:0.0003515057912150203)
4900: accuracy:1.0 loss: 0.63634 (lr:0.00035025140084817447)
4900: ********* epoch 9 ********* test accuracy:0.9863 test loss: 4.6129
4910: accuracy:0.99 loss: 4.90035 (lr:0.0003490032667793838)
4920: accuracy:1.0 loss: 0.615952 (lr:0.0003477613578052316)
4930: accuracy:0.99 loss: 6.24225 (lr:0.0003465256428779287)
4940: accuracy:1.0 loss: 1.30407 (lr:0.00034529609110453763)
4950: accuracy:1.0 loss: 2.80895 (lr:0.00034407267174620007)
4960: accuracy:0.98 loss: 3.9881 (lr:0.0003428553542173683)
4970: accuracy:1.0 loss: 0.86211 (lr:0.0003416441080850407)
4980: accuracy:0.99 loss: 7.96364 (lr:0.00034043890306800073)
4990: accuracy:1.0 loss: 1.17203 (lr:0.00033923970903606046)
5001: accuracy:0.99 loss: 2.56732 (lr:0.00033792750251215517)
5001: ********* epoch 9 ********* test accuracy:0.9858 test loss: 4.69016
max test accuracy: 0.9863

Process finished with exit code 0
