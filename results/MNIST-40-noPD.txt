D:\Anaconda2\envs\py3\envs\tensorflow\python.exe D:/dft/program/predropout-tensorflow/mnist_3.0_convolutional.py
Tensorflow version 1.2.0
Extracting data\train-images-idx3-ubyte.gz
Extracting data\train-labels-idx1-ubyte.gz
Extracting data\t10k-images-idx3-ubyte.gz
Extracting data\t10k-labels-idx1-ubyte.gz
2018-03-09 18:11:18.082598: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.082873: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.083133: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.083397: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.083667: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.083937: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.084207: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.084483: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:11:18.328206: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.645
pciBusID 0000:01:00.0
Total memory: 8.00GiB
Free memory: 6.65GiB
2018-03-09 18:11:18.328513: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0 
2018-03-09 18:11:18.328654: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y 
2018-03-09 18:11:18.328810: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
0: accuracy:0.08 loss: 231.427 (lr:0.003)
0: ********* epoch 1 ********* test accuracy:0.1188 test loss: 230.918
D:\Anaconda2\envs\py3\envs\tensorflow\lib\site-packages\matplotlib\backend_bases.py:2453: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented
  warnings.warn(str, mplDeprecation)
10: accuracy:0.37 loss: 218.747 (lr:0.0029855361896587787)
20: accuracy:0.67 loss: 121.516 (lr:0.0029711445178725875)
30: accuracy:0.77 loss: 68.7217 (lr:0.0029568246248488817)
40: accuracy:0.8 loss: 56.5072 (lr:0.0029425761525895904)
50: accuracy:0.85 loss: 47.2938 (lr:0.0029283987448821647)
60: accuracy:0.88 loss: 39.9363 (lr:0.0029142920472906737)
70: accuracy:0.88 loss: 41.1142 (lr:0.0029002557071469426)
80: accuracy:0.87 loss: 40.5453 (lr:0.0028862893735417373)
90: accuracy:0.91 loss: 39.0641 (lr:0.0028723926973159898)
100: accuracy:0.95 loss: 14.4023 (lr:0.0028585653310520707)
100: ********* epoch 1 ********* test accuracy:0.9203 test loss: 27.5871
110: accuracy:0.89 loss: 40.5206 (lr:0.002844806929065103)
120: accuracy:0.86 loss: 47.9665 (lr:0.0028311171473943213)
130: accuracy:0.95 loss: 19.6975 (lr:0.00281749564379447)
140: accuracy:0.95 loss: 25.1034 (lr:0.0028039420777272502)
150: accuracy:0.94 loss: 19.8213 (lr:0.0027904561103528035)
160: accuracy:0.95 loss: 17.5942 (lr:0.0027770374045212438)
170: accuracy:0.91 loss: 32.8237 (lr:0.0027636856247642266)
180: accuracy:0.86 loss: 31.545 (lr:0.0027504004372865616)
190: accuracy:0.9 loss: 31.8119 (lr:0.002737181509957871)
200: accuracy:0.92 loss: 39.6511 (lr:0.0027240285123042826)
200: ********* epoch 1 ********* test accuracy:0.9381 test loss: 21.574
210: accuracy:0.97 loss: 12.2978 (lr:0.0027109411155001703)
220: accuracy:0.93 loss: 28.6875 (lr:0.0026979189923599317)
230: accuracy:0.89 loss: 29.0662 (lr:0.002684961817329811)
240: accuracy:0.95 loss: 20.996 (lr:0.0026720692664797567)
250: accuracy:0.96 loss: 17.3781 (lr:0.002659241017495327)
260: accuracy:0.95 loss: 21.2728 (lr:0.0026464767496696276)
270: accuracy:0.91 loss: 25.5147 (lr:0.0026337761438952998)
280: accuracy:0.91 loss: 33.7819 (lr:0.002621138882656537)
290: accuracy:0.95 loss: 13.5848 (lr:0.0026085646500211496)
300: accuracy:0.95 loss: 28.8438 (lr:0.0025960531316326675)
300: ********* epoch 1 ********* test accuracy:0.9562 test loss: 14.964
310: accuracy:0.97 loss: 13.0234 (lr:0.002583604014702479)
320: accuracy:0.93 loss: 17.4237 (lr:0.002571216988002013)
330: accuracy:0.96 loss: 15.5758 (lr:0.002558891741854956)
340: accuracy:0.92 loss: 19.3505 (lr:0.002546627968129513)
350: accuracy:0.98 loss: 7.37956 (lr:0.0025344253602307015)
360: accuracy:0.98 loss: 7.79283 (lr:0.0025222836130926888)
370: accuracy:0.93 loss: 23.2873 (lr:0.0025102024231711643)
380: accuracy:0.97 loss: 11.9508 (lr:0.0024981814884357505)
390: accuracy:0.97 loss: 10.2281 (lr:0.0024862205083624536)
400: accuracy:0.96 loss: 11.0808 (lr:0.0024743191839261473)
400: ********* epoch 1 ********* test accuracy:0.9689 test loss: 10.4459
410: accuracy:0.92 loss: 20.109 (lr:0.002462477217593102)
420: accuracy:0.98 loss: 5.21101 (lr:0.0024506943133135424)
430: accuracy:0.97 loss: 11.5025 (lr:0.002438970176514248)
440: accuracy:0.95 loss: 9.92751 (lr:0.0024273045140911875)
450: accuracy:0.99 loss: 13.4563 (lr:0.0024156970344021934)
460: accuracy:0.98 loss: 5.32524 (lr:0.0024041474472596687)
470: accuracy:0.96 loss: 14.0741 (lr:0.0023926554639233334)
480: accuracy:0.95 loss: 14.1866 (lr:0.002381220797093005)
490: accuracy:0.96 loss: 9.09296 (lr:0.0023698431609014176)
500: accuracy:0.99 loss: 7.80787 (lr:0.002358522270907074)
500: ********* epoch 1 ********* test accuracy:0.9682 test loss: 10.5686
510: accuracy:0.99 loss: 4.81326 (lr:0.002347257844087135)
520: accuracy:0.95 loss: 13.6754 (lr:0.0023360495988303423)
530: accuracy:0.98 loss: 8.59567 (lr:0.0023248972549299815)
540: accuracy:0.99 loss: 5.25876 (lr:0.002313800533576874)
550: accuracy:0.99 loss: 5.34046 (lr:0.0023027591573524086)
560: accuracy:0.97 loss: 7.46301 (lr:0.0022917728502216037)
570: accuracy:0.98 loss: 8.87307 (lr:0.0022808413375262097)
580: accuracy:0.94 loss: 12.1526 (lr:0.0022699643459778394)
590: accuracy:0.94 loss: 16.289 (lr:0.0022591416036511374)
600: accuracy:0.97 loss: 10.0773 (lr:0.002248372839976982)
600: ********* epoch 2 ********* test accuracy:0.9635 test loss: 11.9812
610: accuracy:0.99 loss: 4.13424 (lr:0.0022376577857357205)
620: accuracy:0.97 loss: 9.74104 (lr:0.0022269961730504387)
630: accuracy:0.96 loss: 14.8613 (lr:0.0022163877353802647)
640: accuracy:0.95 loss: 11.2229 (lr:0.0022058322075137037)
650: accuracy:0.95 loss: 9.08964 (lr:0.0021953293255620094)
660: accuracy:0.98 loss: 12.0882 (lr:0.0021848788269525857)
670: accuracy:0.99 loss: 6.30163 (lr:0.0021744804504224237)
680: accuracy:0.98 loss: 8.75512 (lr:0.002164133936011568)
690: accuracy:0.97 loss: 15.6649 (lr:0.00215383902505662)
700: accuracy:0.96 loss: 10.0926 (lr:0.002143595460184269)
700: ********* epoch 2 ********* test accuracy:0.973 test loss: 8.94893
710: accuracy:0.99 loss: 5.24892 (lr:0.0021334029853048602)
720: accuracy:0.98 loss: 6.18397 (lr:0.00212326134560599)
730: accuracy:0.99 loss: 4.73341 (lr:0.002113170287546139)
740: accuracy:0.98 loss: 5.31999 (lr:0.0021031295588483287)
750: accuracy:0.99 loss: 6.85469 (lr:0.0020931389084938193)
760: accuracy:0.96 loss: 14.3618 (lr:0.002083198086715832)
770: accuracy:0.97 loss: 6.18891 (lr:0.002073306844993304)
780: accuracy:1.0 loss: 1.1301 (lr:0.0020634649360446776)
790: accuracy:0.96 loss: 13.3861 (lr:0.0020536721138217163)
800: accuracy:0.94 loss: 17.9937 (lr:0.002043928133503354)
800: ********* epoch 2 ********* test accuracy:0.9689 test loss: 9.5261
810: accuracy:0.96 loss: 8.72416 (lr:0.002034232751489576)
820: accuracy:0.98 loss: 4.8784 (lr:0.0020245857253953265)
830: accuracy:0.95 loss: 9.13054 (lr:0.00201498681404445)
840: accuracy:0.98 loss: 9.45418 (lr:0.0020054357774636645)
850: accuracy:0.98 loss: 7.23943 (lr:0.001995932376876557)
860: accuracy:0.99 loss: 3.05472 (lr:0.001986476374697618)
870: accuracy:0.98 loss: 3.92265 (lr:0.0019770675345263007)
880: accuracy:0.99 loss: 3.60826 (lr:0.00196770562114111)
890: accuracy:0.98 loss: 7.1017 (lr:0.0019583904004937245)
900: accuracy:0.99 loss: 6.48781 (lr:0.001949121639703143)
900: ********* epoch 2 ********* test accuracy:0.9773 test loss: 7.63994
910: accuracy:0.98 loss: 5.79798 (lr:0.001939899107049862)
920: accuracy:0.99 loss: 5.03188 (lr:0.0019307225719700854)
930: accuracy:0.98 loss: 7.37174 (lr:0.0019215918050499584)
940: accuracy:0.98 loss: 7.51423 (lr:0.0019125065780198325)
950: accuracy:0.98 loss: 9.65743 (lr:0.0019034666637485586)
960: accuracy:0.97 loss: 5.1528 (lr:0.0018944718362378086)
970: accuracy:0.97 loss: 7.52487 (lr:0.001885521870616427)
980: accuracy:0.97 loss: 11.775 (lr:0.0018766165431348069)
990: accuracy:0.96 loss: 8.84463 (lr:0.001867755631159297)
1000: accuracy:1.0 loss: 1.24072 (lr:0.0018589389131666372)
1000: ********* epoch 2 ********* test accuracy:0.9766 test loss: 7.43497
1010: accuracy:0.98 loss: 9.85511 (lr:0.0018501661687384176)
1020: accuracy:0.97 loss: 7.37895 (lr:0.0018414371785555712)
1030: accuracy:0.98 loss: 4.67787 (lr:0.0018327517243928889)
1040: accuracy:0.96 loss: 6.14992 (lr:0.001824109589113564)
1050: accuracy:0.96 loss: 10.2053 (lr:0.001815510556663764)
1060: accuracy:0.99 loss: 3.6637 (lr:0.0018069544120672303)
1070: accuracy:0.96 loss: 11.8524 (lr:0.001798440941419902)
1080: accuracy:0.97 loss: 10.2241 (lr:0.0017899699318845701)
1090: accuracy:0.99 loss: 9.4859 (lr:0.0017815411716855546)
1100: accuracy:0.98 loss: 4.68204 (lr:0.0017731544501034114)
1100: ********* epoch 2 ********* test accuracy:0.9787 test loss: 6.87044
1110: accuracy:0.98 loss: 6.09244 (lr:0.0017648095574696644)
1120: accuracy:0.98 loss: 5.99535 (lr:0.0017565062851615633)
1130: accuracy:1.0 loss: 1.25917 (lr:0.0017482444255968678)
1140: accuracy:0.95 loss: 15.2017 (lr:0.0017400237722286578)
1150: accuracy:0.96 loss: 10.3035 (lr:0.0017318441195401718)
1160: accuracy:0.97 loss: 5.78708 (lr:0.001723705263039666)
1170: accuracy:0.99 loss: 3.79584 (lr:0.0017156069992553045)
1180: accuracy:0.99 loss: 3.88783 (lr:0.0017075491257300707)
1190: accuracy:0.99 loss: 5.21943 (lr:0.0016995314410167067)
1200: accuracy:0.97 loss: 7.54593 (lr:0.001691553744672677)
1200: ********* epoch 3 ********* test accuracy:0.977 test loss: 7.15513
1210: accuracy:0.97 loss: 7.42633 (lr:0.0016836158372551574)
1220: accuracy:0.97 loss: 11.5278 (lr:0.0016757175203160495)
1230: accuracy:0.98 loss: 6.85387 (lr:0.0016678585963970183)
1240: accuracy:0.98 loss: 11.1643 (lr:0.001660038869024556)
1250: accuracy:0.96 loss: 11.214 (lr:0.001652258142705072)
1260: accuracy:0.98 loss: 6.61471 (lr:0.001644516222920002)
1270: accuracy:0.99 loss: 9.48872 (lr:0.001636812916120949)
1280: accuracy:1.0 loss: 1.91813 (lr:0.001629148029724841)
1290: accuracy:0.98 loss: 3.41306 (lr:0.0016215213721091197)
1300: accuracy:0.97 loss: 12.154 (lr:0.0016139327526069466)
1300: ********* epoch 3 ********* test accuracy:0.9791 test loss: 6.74691
1310: accuracy:0.98 loss: 6.3651 (lr:0.00160638198150244)
1320: accuracy:0.99 loss: 5.59118 (lr:0.0015988688700259279)
1330: accuracy:0.98 loss: 4.22902 (lr:0.0015913932303492327)
1340: accuracy:0.98 loss: 5.90455 (lr:0.0015839548755809732)
1350: accuracy:0.99 loss: 2.96025 (lr:0.0015765536197618927)
1360: accuracy:0.97 loss: 6.58096 (lr:0.0015691892778602098)
1370: accuracy:1.0 loss: 2.30251 (lr:0.0015618616657669942)
1380: accuracy:0.97 loss: 5.99642 (lr:0.0015545706002915614)
1390: accuracy:0.99 loss: 2.99792 (lr:0.0015473158991568946)
1400: accuracy:0.98 loss: 6.20757 (lr:0.0015400973809950877)
1400: ********* epoch 3 ********* test accuracy:0.9791 test loss: 6.35433
1410: accuracy:0.98 loss: 5.66048 (lr:0.001532914865342811)
1420: accuracy:0.99 loss: 3.80119 (lr:0.001525768172636799)
1430: accuracy:0.98 loss: 3.70001 (lr:0.0015186571242093616)
1440: accuracy:1.0 loss: 2.23009 (lr:0.001511581542283918)
1450: accuracy:0.99 loss: 4.516 (lr:0.0015045412499705513)
1460: accuracy:1.0 loss: 0.612028 (lr:0.0014975360712615872)
1470: accuracy:0.97 loss: 9.85359 (lr:0.0014905658310271931)
1480: accuracy:0.98 loss: 5.45765 (lr:0.0014836303550109999)
1490: accuracy:0.96 loss: 5.14971 (lr:0.0014767294698257462)
1500: accuracy:0.99 loss: 2.98203 (lr:0.0014698630029489428)
1500: ********* epoch 3 ********* test accuracy:0.9798 test loss: 6.26738
1510: accuracy:1.0 loss: 1.05318 (lr:0.0014630307827185603)
1520: accuracy:0.99 loss: 1.97538 (lr:0.0014562326383287369)
1530: accuracy:0.99 loss: 3.69045 (lr:0.001449468399825509)
1540: accuracy:0.98 loss: 6.0389 (lr:0.0014427378981025616)
1550: accuracy:0.99 loss: 2.82056 (lr:0.001436040964897001)
1560: accuracy:0.97 loss: 7.24112 (lr:0.0014293774327851483)
1570: accuracy:1.0 loss: 2.2461 (lr:0.0014227471351783538)
1580: accuracy:1.0 loss: 1.63711 (lr:0.001416149906318832)
1590: accuracy:0.98 loss: 5.76848 (lr:0.0014095855812755176)
1600: accuracy:0.97 loss: 5.00001 (lr:0.0014030539959399427)
1600: ********* epoch 3 ********* test accuracy:0.9814 test loss: 6.04633
1610: accuracy:0.99 loss: 1.72794 (lr:0.0013965549870221337)
1620: accuracy:0.99 loss: 4.44758 (lr:0.0013900883920465294)
1630: accuracy:0.98 loss: 10.3607 (lr:0.0013836540493479186)
1640: accuracy:0.99 loss: 2.7229 (lr:0.001377251798067398)
1650: accuracy:0.98 loss: 8.71791 (lr:0.001370881478148353)
1660: accuracy:0.98 loss: 4.30119 (lr:0.0013645429303324535)
1670: accuracy:1.0 loss: 0.823578 (lr:0.0013582359961556737)
1680: accuracy:0.99 loss: 3.16698 (lr:0.0013519605179443314)
1690: accuracy:1.0 loss: 0.994868 (lr:0.0013457163388111437)
1700: accuracy:0.97 loss: 15.5342 (lr:0.0013395033026513076)
1700: ********* epoch 3 ********* test accuracy:0.9831 test loss: 5.55629
1710: accuracy:0.98 loss: 3.29927 (lr:0.001333321254138595)
1720: accuracy:1.0 loss: 1.25888 (lr:0.0013271700387214717)
1730: accuracy:0.99 loss: 3.1029 (lr:0.0013210495026192315)
1740: accuracy:0.97 loss: 9.32913 (lr:0.0013149594928181531)
1750: accuracy:0.99 loss: 2.6637 (lr:0.0013088998570676745)
1760: accuracy:0.98 loss: 4.55113 (lr:0.0013028704438765861)
1770: accuracy:0.97 loss: 15.4047 (lr:0.0012968711025092442)
1780: accuracy:0.98 loss: 7.20955 (lr:0.001290901682981802)
1790: accuracy:0.98 loss: 5.09729 (lr:0.0012849620360584606)
1800: accuracy:1.0 loss: 1.91233 (lr:0.0012790520132477375)
1800: ********* epoch 4 ********* test accuracy:0.9817 test loss: 5.67497
1810: accuracy:0.97 loss: 7.80239 (lr:0.0012731714667987546)
1820: accuracy:0.99 loss: 2.4755 (lr:0.0012673202496975445)
1830: accuracy:0.99 loss: 4.10901 (lr:0.0012614982156633745)
1840: accuracy:1.0 loss: 1.26718 (lr:0.0012557052191450912)
1850: accuracy:0.97 loss: 7.17109 (lr:0.0012499411153174794)
1860: accuracy:0.98 loss: 3.23133 (lr:0.0012442057600776434)
1870: accuracy:0.99 loss: 2.13765 (lr:0.0012384990100414034)
1880: accuracy:0.97 loss: 6.44252 (lr:0.0012328207225397114)
1890: accuracy:0.98 loss: 7.17834 (lr:0.001227170755615084)
1900: accuracy:1.0 loss: 0.878496 (lr:0.0012215489680180538)
1900: ********* epoch 4 ********* test accuracy:0.9818 test loss: 6.12463
1910: accuracy:1.0 loss: 1.7484 (lr:0.001215955219203638)
1920: accuracy:1.0 loss: 1.06438 (lr:0.0012103893693278251)
1930: accuracy:0.96 loss: 6.58508 (lr:0.0012048512792440782)
1940: accuracy:0.99 loss: 2.27026 (lr:0.0011993408104998568)
1950: accuracy:0.97 loss: 7.82943 (lr:0.0011938578253331553)
1960: accuracy:1.0 loss: 0.626579 (lr:0.001188402186669059)
1970: accuracy:0.99 loss: 3.32887 (lr:0.0011829737581163168)
1980: accuracy:0.97 loss: 7.14721 (lr:0.0011775724039639326)
1990: accuracy:0.98 loss: 3.20231 (lr:0.0011721979891777712)
2000: accuracy:0.98 loss: 3.42391 (lr:0.0011668503793971828)
2000: ********* epoch 4 ********* test accuracy:0.9828 test loss: 5.52065
2010: accuracy:0.98 loss: 22.4948 (lr:0.0011615294409316448)
2020: accuracy:0.98 loss: 3.97487 (lr:0.0011562350407574179)
2030: accuracy:0.99 loss: 2.98436 (lr:0.0011509670465142223)
2040: accuracy:0.98 loss: 4.63379 (lr:0.0011457253265019273)
2050: accuracy:0.98 loss: 7.74111 (lr:0.0011405097496772598)
2060: accuracy:0.99 loss: 2.02577 (lr:0.0011353201856505275)
2070: accuracy:0.99 loss: 4.40592 (lr:0.0011301565046823595)
2080: accuracy:0.96 loss: 5.87258 (lr:0.0011250185776804627)
2090: accuracy:0.98 loss: 6.91794 (lr:0.0011199062761963943)
2100: accuracy:0.99 loss: 1.2468 (lr:0.0011148194724223506)
2100: ********* epoch 4 ********* test accuracy:0.9827 test loss: 5.65624
2110: accuracy:0.96 loss: 9.76395 (lr:0.0011097580391879731)
2120: accuracy:0.99 loss: 2.98838 (lr:0.0011047218499571666)
2130: accuracy:0.97 loss: 5.17058 (lr:0.0010997107788249386)
2140: accuracy:0.97 loss: 7.35025 (lr:0.0010947247005142493)
2150: accuracy:1.0 loss: 1.44999 (lr:0.001089763490372882)
2160: accuracy:0.98 loss: 4.3018 (lr:0.0010848270243703235)
2170: accuracy:0.99 loss: 6.63047 (lr:0.001079915179094668)
2180: accuracy:0.99 loss: 2.95492 (lr:0.0010750278317495268)
2190: accuracy:0.99 loss: 2.98128 (lr:0.0010701648601509621)
2200: accuracy:1.0 loss: 0.524157 (lr:0.0010653261427244307)
2200: ********* epoch 4 ********* test accuracy:0.9829 test loss: 5.42599
2210: accuracy:1.0 loss: 1.13704 (lr:0.001060511558501745)
2220: accuracy:0.99 loss: 3.95613 (lr:0.0010557209871180483)
2230: accuracy:0.99 loss: 2.88687 (lr:0.0010509543088088069)
2240: accuracy:0.99 loss: 4.97616 (lr:0.0010462114044068145)
2250: accuracy:0.99 loss: 1.52842 (lr:0.0010414921553392143)
2260: accuracy:0.98 loss: 4.63159 (lr:0.0010367964436245336)
2270: accuracy:1.0 loss: 1.52759 (lr:0.001032124151869735)
2280: accuracy:0.99 loss: 1.57236 (lr:0.0010274751632672816)
2290: accuracy:0.97 loss: 9.08516 (lr:0.0010228493615922153)
2300: accuracy:0.99 loss: 4.50909 (lr:0.0010182466311992545)
2300: ********* epoch 4 ********* test accuracy:0.9819 test loss: 5.82181
2310: accuracy:0.98 loss: 7.87194 (lr:0.0010136668570198987)
2320: accuracy:0.97 loss: 4.14534 (lr:0.0010091099245595554)
2330: accuracy:0.99 loss: 2.36321 (lr:0.0010045757198946755)
2340: accuracy:1.0 loss: 1.38382 (lr:0.0010000641296699067)
2350: accuracy:0.99 loss: 2.63386 (lr:0.0009955750410952577)
2360: accuracy:0.99 loss: 7.11962 (lr:0.0009911083419432806)
2370: accuracy:0.98 loss: 5.99557 (lr:0.000986663920546264)
2380: accuracy:0.99 loss: 1.95913 (lr:0.0009822416657934419)
2390: accuracy:0.99 loss: 1.81359 (lr:0.0009778414671282143)
2400: accuracy:0.96 loss: 8.1188 (lr:0.0009734632145453863)
2400: ********* epoch 5 ********* test accuracy:0.9841 test loss: 5.2166
2410: accuracy:1.0 loss: 0.226722 (lr:0.0009691067985884144)
2420: accuracy:0.98 loss: 4.40399 (lr:0.0009647721103466736)
2430: accuracy:0.99 loss: 2.21366 (lr:0.0009604590414527313)
2440: accuracy:1.0 loss: 0.957013 (lr:0.0009561674840796413)
2450: accuracy:1.0 loss: 1.6774 (lr:0.0009518973309382452)
2460: accuracy:0.99 loss: 5.91654 (lr:0.0009476484752744924)
2470: accuracy:0.99 loss: 2.41779 (lr:0.0009434208108667696)
2480: accuracy:0.98 loss: 3.86829 (lr:0.0009392142320232469)
2490: accuracy:1.0 loss: 0.595581 (lr:0.0009350286335792337)
2500: accuracy:0.98 loss: 4.57071 (lr:0.0009308639108945514)
2500: ********* epoch 5 ********* test accuracy:0.9825 test loss: 5.14396
2510: accuracy:0.99 loss: 3.12664 (lr:0.0009267199598509155)
2520: accuracy:0.99 loss: 3.71947 (lr:0.0009225966768493343)
2530: accuracy:1.0 loss: 0.788785 (lr:0.0009184939588075178)
2540: accuracy:0.98 loss: 4.64599 (lr:0.0009144117031573014)
2550: accuracy:0.98 loss: 7.66296 (lr:0.0009103498078420814)
2560: accuracy:0.98 loss: 3.31649 (lr:0.0009063081713142631)
2570: accuracy:0.97 loss: 7.5292 (lr:0.0009022866925327231)
2580: accuracy:1.0 loss: 0.374628 (lr:0.0008982852709602818)
2590: accuracy:0.99 loss: 4.51377 (lr:0.0008943038065611923)
2600: accuracy:1.0 loss: 0.753924 (lr:0.0008903421997986366)
2600: ********* epoch 5 ********* test accuracy:0.9838 test loss: 4.85618
2610: accuracy:0.99 loss: 2.49958 (lr:0.0008864003516322398)
2620: accuracy:1.0 loss: 1.41308 (lr:0.0008824781635155918)
2630: accuracy:0.99 loss: 2.235 (lr:0.0008785755373937862)
2640: accuracy:1.0 loss: 0.373976 (lr:0.000874692375700966)
2650: accuracy:0.99 loss: 2.62826 (lr:0.0008708285813578872)
2660: accuracy:1.0 loss: 0.65471 (lr:0.0008669840577694896)
2670: accuracy:1.0 loss: 0.239406 (lr:0.0008631587088224834)
2680: accuracy:0.99 loss: 4.17057 (lr:0.0008593524388829455)
2690: accuracy:0.99 loss: 1.93949 (lr:0.0008555651527939294)
2700: accuracy:1.0 loss: 2.56092 (lr:0.0008517967558730855)
2700: ********* epoch 5 ********* test accuracy:0.9845 test loss: 5.15315
2710: accuracy:0.99 loss: 6.40168 (lr:0.0008480471539102946)
2720: accuracy:0.99 loss: 2.81582 (lr:0.0008443162531653122)
2730: accuracy:0.98 loss: 7.04419 (lr:0.0008406039603654254)
2740: accuracy:1.0 loss: 1.40389 (lr:0.0008369101827031209)
2750: accuracy:0.99 loss: 3.58341 (lr:0.0008332348278337648)
2760: accuracy:0.99 loss: 3.15588 (lr:0.000829577803873294)
2770: accuracy:0.98 loss: 3.16275 (lr:0.0008259390193959188)
2780: accuracy:1.0 loss: 2.07195 (lr:0.000822318383431838)
2790: accuracy:1.0 loss: 1.95333 (lr:0.0008187158054649635)
2800: accuracy:1.0 loss: 0.385882 (lr:0.0008151311954306589)
2800: ********* epoch 5 ********* test accuracy:0.9846 test loss: 4.96444
2810: accuracy:0.98 loss: 8.12824 (lr:0.0008115644637134865)
2820: accuracy:0.99 loss: 2.79223 (lr:0.0008080155211449677)
2830: accuracy:0.99 loss: 1.33664 (lr:0.0008044842790013532)
2840: accuracy:1.0 loss: 0.656994 (lr:0.0008009706490014058)
2850: accuracy:0.99 loss: 4.20404 (lr:0.0007974745433041923)
2860: accuracy:1.0 loss: 0.375508 (lr:0.0007939958745068883)
2870: accuracy:0.99 loss: 6.55212 (lr:0.0007905345556425924)
2880: accuracy:0.99 loss: 4.35221 (lr:0.0007870905001781532)
2890: accuracy:0.99 loss: 9.54364 (lr:0.0007836636220120043)
2900: accuracy:0.99 loss: 2.07364 (lr:0.0007802538354720133)
2900: ********* epoch 5 ********* test accuracy:0.985 test loss: 4.71651
2910: accuracy:0.99 loss: 7.56339 (lr:0.000776861055313339)
2920: accuracy:0.97 loss: 6.39749 (lr:0.0007734851967163007)
2930: accuracy:0.98 loss: 3.21036 (lr:0.0007701261752842576)
2940: accuracy:0.99 loss: 3.27869 (lr:0.0007667839070414992)
2950: accuracy:1.0 loss: 1.2385 (lr:0.0007634583084311452)
2960: accuracy:1.0 loss: 1.85761 (lr:0.000760149296313057)
2970: accuracy:1.0 loss: 0.748741 (lr:0.0007568567879617594)
2980: accuracy:1.0 loss: 1.75157 (lr:0.0007535807010643724)
2990: accuracy:0.98 loss: 23.9588 (lr:0.0007503209537185525)
3000: accuracy:1.0 loss: 0.289749 (lr:0.0007470774644304465)
3000: ********* epoch 6 ********* test accuracy:0.9839 test loss: 4.98704
3010: accuracy:0.99 loss: 4.90622 (lr:0.0007438501521126534)
3020: accuracy:1.0 loss: 1.71284 (lr:0.0007406389360821968)
3030: accuracy:1.0 loss: 1.45122 (lr:0.0007374437360585091)
3040: accuracy:0.98 loss: 4.87408 (lr:0.0007342644721614229)
3050: accuracy:1.0 loss: 1.20882 (lr:0.0007311010649091755)
3060: accuracy:0.99 loss: 3.22248 (lr:0.0007279534352164206)
3070: accuracy:0.98 loss: 5.54299 (lr:0.0007248215043922522)
3080: accuracy:1.0 loss: 0.602953 (lr:0.0007217051941382361)
3090: accuracy:0.98 loss: 6.39779 (lr:0.0007186044265464543)
3100: accuracy:0.99 loss: 1.959 (lr:0.0007155191240975549)
3100: ********* epoch 6 ********* test accuracy:0.9832 test loss: 5.07882
3110: accuracy:0.97 loss: 11.5076 (lr:0.0007124492096588165)
3120: accuracy:0.98 loss: 4.22219 (lr:0.0007093946064822178)
3130: accuracy:1.0 loss: 1.56091 (lr:0.0007063552382025206)
3140: accuracy:0.98 loss: 5.12805 (lr:0.0007033310288353594)
3150: accuracy:0.99 loss: 1.07254 (lr:0.0007003219027753427)
3160: accuracy:1.0 loss: 0.733756 (lr:0.000697327784794162)
3170: accuracy:1.0 loss: 0.803095 (lr:0.0006943486000387123)
3180: accuracy:1.0 loss: 1.61821 (lr:0.000691384274029219)
3190: accuracy:0.99 loss: 2.50019 (lr:0.0006884347326573779)
3200: accuracy:0.99 loss: 1.81922 (lr:0.0006854999021845007)
3200: ********* epoch 6 ********* test accuracy:0.9846 test loss: 5.09304
3210: accuracy:1.0 loss: 0.421344 (lr:0.0006825797092396732)
3220: accuracy:0.97 loss: 15.4343 (lr:0.0006796740808179191)
3230: accuracy:0.99 loss: 2.87719 (lr:0.000676782944278377)
3240: accuracy:1.0 loss: 1.09107 (lr:0.0006739062273424826)
3250: accuracy:0.99 loss: 1.76876 (lr:0.0006710438580921628)
3260: accuracy:0.99 loss: 3.46044 (lr:0.0006681957649680373)
3270: accuracy:0.99 loss: 2.30437 (lr:0.0006653618767676294)
3280: accuracy:1.0 loss: 0.488248 (lr:0.0006625421226435866)
3290: accuracy:1.0 loss: 0.92136 (lr:0.0006597364321019091)
3300: accuracy:0.99 loss: 6.24369 (lr:0.000656944735000187)
3300: ********* epoch 6 ********* test accuracy:0.9848 test loss: 5.09529
3310: accuracy:1.0 loss: 1.01318 (lr:0.0006541669615458476)
3320: accuracy:0.99 loss: 4.56517 (lr:0.0006514030422944097)
3330: accuracy:1.0 loss: 0.951574 (lr:0.000648652908147748)
3340: accuracy:0.99 loss: 1.83893 (lr:0.0006459164903523658)
3350: accuracy:0.99 loss: 2.6821 (lr:0.0006431937204976754)
3360: accuracy:0.99 loss: 0.986163 (lr:0.000640484530514289)
3370: accuracy:0.98 loss: 3.23469 (lr:0.0006377888526723156)
3380: accuracy:0.98 loss: 7.00874 (lr:0.000635106619579669)
3390: accuracy:0.99 loss: 2.72347 (lr:0.0006324377641803819)
3400: accuracy:0.99 loss: 4.19064 (lr:0.0006297822197529307)
3400: ********* epoch 6 ********* test accuracy:0.9846 test loss: 4.78352
3410: accuracy:0.99 loss: 2.91876 (lr:0.0006271399199085659)
3420: accuracy:1.0 loss: 0.321157 (lr:0.0006245107985896541)
3430: accuracy:1.0 loss: 1.59828 (lr:0.0006218947900680254)
3440: accuracy:1.0 loss: 0.520895 (lr:0.0006192918289433304)
3450: accuracy:1.0 loss: 0.51869 (lr:0.0006167018501414055)
3460: accuracy:1.0 loss: 0.927324 (lr:0.0006141247889126458)
3470: accuracy:1.0 loss: 0.505664 (lr:0.0006115605808303861)
3480: accuracy:0.99 loss: 7.37508 (lr:0.000609009161789291)
3490: accuracy:1.0 loss: 1.31578 (lr:0.0006064704680037515)
3500: accuracy:0.99 loss: 4.14237 (lr:0.000603944436006291)
3500: ********* epoch 6 ********* test accuracy:0.9855 test loss: 4.63586
3510: accuracy:0.99 loss: 1.68301 (lr:0.0006014310026459777)
3520: accuracy:1.0 loss: 0.419752 (lr:0.0005989301050868467)
3530: accuracy:1.0 loss: 0.456939 (lr:0.0005964416808063288)
3540: accuracy:0.98 loss: 5.31684 (lr:0.0005939656675936874)
3550: accuracy:0.99 loss: 1.86837 (lr:0.0005915020035484634)
3560: accuracy:0.98 loss: 4.86041 (lr:0.000589050627078927)
3570: accuracy:0.99 loss: 4.64792 (lr:0.0005866114769005391)
3580: accuracy:1.0 loss: 1.13552 (lr:0.000584184492034418)
3590: accuracy:0.99 loss: 2.90268 (lr:0.000581769611805816)
3600: accuracy:1.0 loss: 0.917294 (lr:0.000579366775842601)
3600: ********* epoch 7 ********* test accuracy:0.9844 test loss: 4.69198
3610: accuracy:1.0 loss: 0.728158 (lr:0.0005769759240737492)
3620: accuracy:1.0 loss: 0.906873 (lr:0.0005745969967278418)
3630: accuracy:1.0 loss: 0.642062 (lr:0.0005722299343315713)
3640: accuracy:1.0 loss: 0.869563 (lr:0.0005698746777082543)
3650: accuracy:1.0 loss: 0.155735 (lr:0.0005675311679763527)
3660: accuracy:1.0 loss: 0.789991 (lr:0.000565199346548001)
3670: accuracy:1.0 loss: 1.34336 (lr:0.0005628791551275424)
3680: accuracy:0.99 loss: 2.54366 (lr:0.00056057053571007)
3690: accuracy:1.0 loss: 0.240635 (lr:0.0005582734305799787)
3700: accuracy:0.98 loss: 3.05743 (lr:0.0005559877823095201)
3700: ********* epoch 7 ********* test accuracy:0.9852 test loss: 4.83419
3710: accuracy:0.97 loss: 9.73987 (lr:0.0005537135337573688)
3720: accuracy:1.0 loss: 0.177085 (lr:0.0005514506280671922)
3730: accuracy:1.0 loss: 0.112067 (lr:0.0005491990086662305)
3740: accuracy:0.99 loss: 2.55488 (lr:0.0005469586192638811)
3750: accuracy:0.98 loss: 6.83959 (lr:0.0005447294038502926)
3760: accuracy:1.0 loss: 0.374563 (lr:0.0005425113066949633)
3770: accuracy:1.0 loss: 1.00341 (lr:0.0005403042723453488)
3780: accuracy:1.0 loss: 0.399884 (lr:0.0005381082456254755)
3790: accuracy:0.99 loss: 3.82571 (lr:0.0005359231716345609)
3800: accuracy:1.0 loss: 0.297579 (lr:0.0005337489957456418)
3800: ********* epoch 7 ********* test accuracy:0.9855 test loss: 4.99715
3810: accuracy:1.0 loss: 1.21897 (lr:0.0005315856636042072)
3820: accuracy:0.99 loss: 1.52183 (lr:0.0005294331211268412)
3830: accuracy:1.0 loss: 0.554378 (lr:0.0005272913144998697)
3840: accuracy:0.97 loss: 25.5923 (lr:0.0005251601901780155)
3850: accuracy:1.0 loss: 1.57768 (lr:0.0005230396948830594)
3860: accuracy:0.98 loss: 2.48096 (lr:0.0005209297756025089)
3870: accuracy:0.98 loss: 5.06988 (lr:0.0005188303795882718)
3880: accuracy:1.0 loss: 0.861225 (lr:0.0005167414543553385)
3890: accuracy:0.97 loss: 12.173 (lr:0.0005146629476804694)
3900: accuracy:1.0 loss: 0.654495 (lr:0.0005125948076008895)
3900: ********* epoch 7 ********* test accuracy:0.9854 test loss: 4.79787
3910: accuracy:0.99 loss: 2.93201 (lr:0.0005105369824129888)
3920: accuracy:0.99 loss: 4.06676 (lr:0.0005084894206710306)
3930: accuracy:1.0 loss: 1.51842 (lr:0.0005064520711858646)
3940: accuracy:1.0 loss: 0.561807 (lr:0.0005044248830236478)
3950: accuracy:1.0 loss: 0.315618 (lr:0.0005024078055045702)
3960: accuracy:0.99 loss: 7.84655 (lr:0.0005004007882015892)
3970: accuracy:0.99 loss: 1.34811 (lr:0.0004984037809391673)
3980: accuracy:0.99 loss: 2.63019 (lr:0.0004964167337920192)
3990: accuracy:0.99 loss: 1.28245 (lr:0.0004944395970838627)
4000: accuracy:0.99 loss: 2.83881 (lr:0.0004924723213861769)
4000: ********* epoch 7 ********* test accuracy:0.9865 test loss: 4.64014
4010: accuracy:1.0 loss: 0.34958 (lr:0.000490514857516967)
4020: accuracy:1.0 loss: 0.770685 (lr:0.0004885671565395345)
4030: accuracy:1.0 loss: 0.857835 (lr:0.00048662916976125316)
4040: accuracy:1.0 loss: 0.313358 (lr:0.00048470084873235297)
4050: accuracy:1.0 loss: 0.354794 (lr:0.00048278214524470766)
4060: accuracy:0.99 loss: 5.96164 (lr:0.00048087301133063005)
4070: accuracy:1.0 loss: 1.68623 (lr:0.0004789733992616726)
4080: accuracy:0.98 loss: 2.46318 (lr:0.00047708326154743513)
4090: accuracy:0.99 loss: 4.43052 (lr:0.00047520255093437617)
4100: accuracy:1.0 loss: 0.104025 (lr:0.0004733312204046323)
4100: ********* epoch 7 ********* test accuracy:0.9865 test loss: 4.61186
4110: accuracy:0.98 loss: 5.95513 (lr:0.0004714692231748428)
4120: accuracy:0.98 loss: 5.72503 (lr:0.0004696165126949802)
4130: accuracy:0.94 loss: 16.2664 (lr:0.0004677730426471858)
4140: accuracy:1.0 loss: 1.12127 (lr:0.00046593876694461247)
4150: accuracy:1.0 loss: 0.790122 (lr:0.0004641136397302719)
4160: accuracy:0.97 loss: 7.62946 (lr:0.000462297615375889)
4170: accuracy:1.0 loss: 1.09862 (lr:0.0004604906484807602)
4180: accuracy:1.0 loss: 0.1384 (lr:0.000458692693870619)
4190: accuracy:1.0 loss: 0.793866 (lr:0.0004569037065965064)
4200: accuracy:0.99 loss: 1.30612 (lr:0.00045512364193364755)
4200: ********* epoch 8 ********* test accuracy:0.9851 test loss: 4.62142
4210: accuracy:1.0 loss: 1.19583 (lr:0.00045335245538033307)
4220: accuracy:1.0 loss: 0.79806 (lr:0.0004515901026568069)
4230: accuracy:0.99 loss: 2.03671 (lr:0.000449836539704159)
4240: accuracy:1.0 loss: 0.831046 (lr:0.00044809172268322453)
4250: accuracy:1.0 loss: 0.224602 (lr:0.00044635560797348693)
4260: accuracy:0.99 loss: 2.62763 (lr:0.00044462815217198804)
4270: accuracy:1.0 loss: 0.0940716 (lr:0.0004429093120922428)
4280: accuracy:1.0 loss: 0.517769 (lr:0.0004411990447631597)
4290: accuracy:1.0 loss: 1.72343 (lr:0.0004394973074279665)
4300: accuracy:1.0 loss: 0.234644 (lr:0.00043780405754314123)
4300: ********* epoch 8 ********* test accuracy:0.9858 test loss: 4.61064
4310: accuracy:1.0 loss: 0.454278 (lr:0.00043611925277734854)
4320: accuracy:1.0 loss: 0.722077 (lr:0.0004344428510103813)
4330: accuracy:1.0 loss: 1.27978 (lr:0.0004327748103321084)
4340: accuracy:1.0 loss: 1.02542 (lr:0.00043111508904142585)
4350: accuracy:1.0 loss: 0.580321 (lr:0.00042946364564521496)
4360: accuracy:1.0 loss: 0.637059 (lr:0.0004278204388573046)
4370: accuracy:0.99 loss: 2.1742 (lr:0.0004261854275974398)
4380: accuracy:0.99 loss: 7.25908 (lr:0.0004245585709902538)
4390: accuracy:1.0 loss: 0.485039 (lr:0.0004229398283642466)
4400: accuracy:0.99 loss: 1.28523 (lr:0.00042132915925076824)
4400: ********* epoch 8 ********* test accuracy:0.9855 test loss: 4.79467
4410: accuracy:0.99 loss: 2.42822 (lr:0.00041972652338300717)
4420: accuracy:1.0 loss: 0.240532 (lr:0.0004181318806949831)
4430: accuracy:0.99 loss: 2.1753 (lr:0.0004165451913205458)
4440: accuracy:1.0 loss: 0.107975 (lr:0.0004149664155923781)
4450: accuracy:0.99 loss: 2.02453 (lr:0.00041339551404100485)
4460: accuracy:1.0 loss: 0.368698 (lr:0.0004118324473938054)
4470: accuracy:0.99 loss: 2.72267 (lr:0.00041027717657403205)
4480: accuracy:0.99 loss: 2.20233 (lr:0.00040872966269983314)
4490: accuracy:0.99 loss: 1.60781 (lr:0.00040718986708328155)
4500: accuracy:1.0 loss: 0.788844 (lr:0.00040565775122940656)
4500: ********* epoch 8 ********* test accuracy:0.9868 test loss: 4.86932
4510: accuracy:0.99 loss: 2.49106 (lr:0.000404133276835232)
4520: accuracy:1.0 loss: 0.719671 (lr:0.0004026164057888186)
4530: accuracy:1.0 loss: 0.331841 (lr:0.0004011071001683111)
4540: accuracy:0.99 loss: 3.71192 (lr:0.0003996053222409906)
4550: accuracy:1.0 loss: 0.957346 (lr:0.00039811103446233054)
4560: accuracy:0.99 loss: 1.6655 (lr:0.0003966241994750587)
4570: accuracy:0.98 loss: 6.7396 (lr:0.0003951447801082228)
4580: accuracy:1.0 loss: 1.36397 (lr:0.00039367273937626184)
4590: accuracy:1.0 loss: 0.219706 (lr:0.00039220804047808086)
4600: accuracy:1.0 loss: 0.704311 (lr:0.0003907506467961309)
4600: ********* epoch 8 ********* test accuracy:0.9853 test loss: 4.93598
4610: accuracy:0.99 loss: 2.06489 (lr:0.00038930052189549403)
4620: accuracy:1.0 loss: 0.656142 (lr:0.0003878576295229724)
4630: accuracy:1.0 loss: 0.778021 (lr:0.0003864219336061815)
4640: accuracy:1.0 loss: 0.897802 (lr:0.0003849933982526485)
4650: accuracy:1.0 loss: 1.39081 (lr:0.00038357198774891513)
4660: accuracy:1.0 loss: 0.696134 (lr:0.00038215766655964504)
4670: accuracy:1.0 loss: 1.4421 (lr:0.0003807503993267346)
4680: accuracy:1.0 loss: 1.63437 (lr:0.0003793501508684298)
4690: accuracy:1.0 loss: 0.623434 (lr:0.0003779568861784461)
4700: accuracy:1.0 loss: 1.08068 (lr:0.0003765705704250939)
4700: ********* epoch 8 ********* test accuracy:0.9856 test loss: 4.65392
4710: accuracy:1.0 loss: 0.214929 (lr:0.00037519116895040703)
4720: accuracy:1.0 loss: 1.31659 (lr:0.0003738186472692768)
4730: accuracy:0.99 loss: 1.60238 (lr:0.00037245297106858964)
4740: accuracy:1.0 loss: 0.779946 (lr:0.0003710941062063696)
4750: accuracy:0.98 loss: 4.50256 (lr:0.00036974201871092414)
4760: accuracy:1.0 loss: 0.683215 (lr:0.00036839667477999555)
4770: accuracy:0.99 loss: 6.28435 (lr:0.0003670580407799155)
4780: accuracy:1.0 loss: 0.883994 (lr:0.00036572608324476403)
4790: accuracy:1.0 loss: 1.35845 (lr:0.0003644007688755338)
4800: accuracy:1.0 loss: 0.452797 (lr:0.0003630820645392963)
4800: ********* epoch 9 ********* test accuracy:0.985 test loss: 4.86614
4810: accuracy:1.0 loss: 0.944972 (lr:0.0003617699372683745)
4820: accuracy:0.99 loss: 2.35618 (lr:0.00036046435425951816)
4830: accuracy:1.0 loss: 0.722259 (lr:0.00035916528287308427)
4840: accuracy:1.0 loss: 0.862876 (lr:0.00035787269063222043)
4850: accuracy:1.0 loss: 0.346766 (lr:0.0003565865452220532)
4860: accuracy:1.0 loss: 0.284353 (lr:0.0003553068144888804)
4870: accuracy:1.0 loss: 0.43876 (lr:0.00035403346643936713)
4880: accuracy:1.0 loss: 0.207183 (lr:0.00035276646923974576)
4890: accuracy:1.0 loss: 0.986116 (lr:0.0003515057912150203)
4900: accuracy:1.0 loss: 0.510284 (lr:0.00035025140084817447)
4900: ********* epoch 9 ********* test accuracy:0.9856 test loss: 4.68045
4910: accuracy:1.0 loss: 0.842811 (lr:0.0003490032667793838)
4920: accuracy:1.0 loss: 0.0958486 (lr:0.0003477613578052316)
4930: accuracy:0.99 loss: 2.26778 (lr:0.0003465256428779287)
4940: accuracy:1.0 loss: 0.520566 (lr:0.00034529609110453763)
4950: accuracy:1.0 loss: 0.473638 (lr:0.00034407267174620007)
4960: accuracy:0.98 loss: 3.11981 (lr:0.0003428553542173683)
4970: accuracy:1.0 loss: 0.150546 (lr:0.0003416441080850407)
4980: accuracy:0.99 loss: 3.94757 (lr:0.00034043890306800073)
4990: accuracy:0.99 loss: 1.26979 (lr:0.00033923970903606046)
5001: accuracy:1.0 loss: 0.932683 (lr:0.00033792750251215517)
5001: ********* epoch 9 ********* test accuracy:0.9856 test loss: 4.75431
max test accuracy: 0.9868

Process finished with exit code 0
