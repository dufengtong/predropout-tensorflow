D:\Anaconda2\envs\py3\envs\tensorflow\python.exe D:/dft/program/predropout-tensorflow/mnist_3.0_convolutional.py
Tensorflow version 1.2.0
Extracting data\train-images-idx3-ubyte.gz
Extracting data\train-labels-idx1-ubyte.gz
Extracting data\t10k-images-idx3-ubyte.gz
Extracting data\t10k-labels-idx1-ubyte.gz
2018-03-09 18:03:28.771927: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.772213: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.772472: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.772734: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.773007: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.773277: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.773545: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:28.773808: W c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\platform\cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-03-09 18:03:29.018541: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.645
pciBusID 0000:01:00.0
Total memory: 8.00GiB
Free memory: 6.65GiB
2018-03-09 18:03:29.018837: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:961] DMA: 0 
2018-03-09 18:03:29.018978: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:971] 0:   Y 
2018-03-09 18:03:29.019132: I c:\tf_jenkins\home\workspace\release-win\m\windows-gpu\py\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
0: accuracy:0.07 loss: 236.574 (lr:0.003)
0: ********* epoch 1 ********* test accuracy:0.0955 test loss: 236.163
D:\Anaconda2\envs\py3\envs\tensorflow\lib\site-packages\matplotlib\backend_bases.py:2453: MatplotlibDeprecationWarning: Using default event loop until function specific to this GUI is implemented
  warnings.warn(str, mplDeprecation)
10: accuracy:0.52 loss: 192.277 (lr:0.0029855361896587787)
20: accuracy:0.79 loss: 69.5069 (lr:0.0029711445178725875)
30: accuracy:0.81 loss: 71.1796 (lr:0.0029568246248488817)
40: accuracy:0.86 loss: 38.7532 (lr:0.0029425761525895904)
50: accuracy:0.9 loss: 33.4796 (lr:0.0029283987448821647)
60: accuracy:0.89 loss: 30.7719 (lr:0.0029142920472906737)
70: accuracy:0.93 loss: 30.0712 (lr:0.0029002557071469426)
80: accuracy:0.89 loss: 31.7013 (lr:0.0028862893735417373)
90: accuracy:0.91 loss: 31.006 (lr:0.0028723926973159898)
100: accuracy:0.98 loss: 13.2863 (lr:0.0028585653310520707)
100: ********* epoch 1 ********* test accuracy:0.9347 test loss: 22.8147
110: accuracy:0.91 loss: 30.5176 (lr:0.002844806929065103)
120: accuracy:0.93 loss: 40.6623 (lr:0.0028311171473943213)
130: accuracy:0.97 loss: 10.864 (lr:0.00281749564379447)
140: accuracy:0.93 loss: 18.9408 (lr:0.0028039420777272502)
150: accuracy:0.95 loss: 15.4193 (lr:0.0027904561103528035)
160: accuracy:0.95 loss: 13.0601 (lr:0.0027770374045212438)
170: accuracy:0.94 loss: 25.1605 (lr:0.0027636856247642266)
180: accuracy:0.93 loss: 19.9211 (lr:0.0027504004372865616)
190: accuracy:0.93 loss: 22.8628 (lr:0.002737181509957871)
200: accuracy:0.91 loss: 31.4092 (lr:0.0027240285123042826)
200: ********* epoch 1 ********* test accuracy:0.9511 test loss: 15.8386
210: accuracy:0.97 loss: 12.8155 (lr:0.0027109411155001703)
220: accuracy:0.95 loss: 25.1537 (lr:0.0026979189923599317)
230: accuracy:0.94 loss: 19.8419 (lr:0.002684961817329811)
240: accuracy:0.96 loss: 16.2872 (lr:0.0026720692664797567)
250: accuracy:0.97 loss: 16.7675 (lr:0.002659241017495327)
260: accuracy:0.96 loss: 14.8964 (lr:0.0026464767496696276)
270: accuracy:0.93 loss: 24.9362 (lr:0.0026337761438952998)
280: accuracy:0.94 loss: 21.1564 (lr:0.002621138882656537)
290: accuracy:0.97 loss: 12.003 (lr:0.0026085646500211496)
300: accuracy:0.95 loss: 20.8667 (lr:0.0025960531316326675)
300: ********* epoch 1 ********* test accuracy:0.966 test loss: 10.7505
310: accuracy:0.96 loss: 10.1422 (lr:0.002583604014702479)
320: accuracy:0.96 loss: 11.0756 (lr:0.002571216988002013)
330: accuracy:0.96 loss: 11.6126 (lr:0.002558891741854956)
340: accuracy:0.95 loss: 9.74634 (lr:0.002546627968129513)
350: accuracy:1.0 loss: 4.17614 (lr:0.0025344253602307015)
360: accuracy:1.0 loss: 3.76218 (lr:0.0025222836130926888)
370: accuracy:0.94 loss: 22.8411 (lr:0.0025102024231711643)
380: accuracy:0.97 loss: 8.18841 (lr:0.0024981814884357505)
390: accuracy:0.96 loss: 15.4554 (lr:0.0024862205083624536)
400: accuracy:0.96 loss: 16.1684 (lr:0.0024743191839261473)
400: ********* epoch 1 ********* test accuracy:0.9758 test loss: 7.71371
410: accuracy:0.94 loss: 19.6637 (lr:0.002462477217593102)
420: accuracy:1.0 loss: 2.05753 (lr:0.0024506943133135424)
430: accuracy:0.97 loss: 7.61939 (lr:0.002438970176514248)
440: accuracy:0.99 loss: 5.12663 (lr:0.0024273045140911875)
450: accuracy:0.99 loss: 8.31881 (lr:0.0024156970344021934)
460: accuracy:0.99 loss: 3.35078 (lr:0.0024041474472596687)
470: accuracy:0.97 loss: 13.2056 (lr:0.0023926554639233334)
480: accuracy:0.96 loss: 6.44399 (lr:0.002381220797093005)
490: accuracy:0.97 loss: 7.15275 (lr:0.0023698431609014176)
500: accuracy:0.99 loss: 6.27201 (lr:0.002358522270907074)
500: ********* epoch 1 ********* test accuracy:0.9769 test loss: 7.13285
510: accuracy:0.99 loss: 4.41497 (lr:0.002347257844087135)
520: accuracy:0.96 loss: 14.3984 (lr:0.0023360495988303423)
530: accuracy:0.97 loss: 9.12514 (lr:0.0023248972549299815)
540: accuracy:0.98 loss: 4.10874 (lr:0.002313800533576874)
550: accuracy:0.98 loss: 3.56405 (lr:0.0023027591573524086)
560: accuracy:0.95 loss: 10.6292 (lr:0.0022917728502216037)
570: accuracy:0.98 loss: 9.31805 (lr:0.0022808413375262097)
580: accuracy:0.97 loss: 10.5564 (lr:0.0022699643459778394)
590: accuracy:0.95 loss: 16.8194 (lr:0.0022591416036511374)
600: accuracy:0.96 loss: 9.70325 (lr:0.002248372839976982)
600: ********* epoch 2 ********* test accuracy:0.9789 test loss: 6.36279
610: accuracy:0.98 loss: 4.6687 (lr:0.0022376577857357205)
620: accuracy:0.97 loss: 8.19936 (lr:0.0022269961730504387)
630: accuracy:0.96 loss: 9.76766 (lr:0.0022163877353802647)
640: accuracy:0.97 loss: 6.67393 (lr:0.0022058322075137037)
650: accuracy:0.99 loss: 4.31303 (lr:0.0021953293255620094)
660: accuracy:0.99 loss: 5.86425 (lr:0.0021848788269525857)
670: accuracy:0.98 loss: 6.73663 (lr:0.0021744804504224237)
680: accuracy:0.98 loss: 4.89671 (lr:0.002164133936011568)
690: accuracy:0.97 loss: 14.8305 (lr:0.00215383902505662)
700: accuracy:0.98 loss: 5.83517 (lr:0.002143595460184269)
700: ********* epoch 2 ********* test accuracy:0.983 test loss: 5.19896
710: accuracy:1.0 loss: 1.15352 (lr:0.0021334029853048602)
720: accuracy:0.98 loss: 5.1473 (lr:0.00212326134560599)
730: accuracy:0.99 loss: 2.64302 (lr:0.002113170287546139)
740: accuracy:1.0 loss: 2.01128 (lr:0.0021031295588483287)
750: accuracy:0.99 loss: 7.71038 (lr:0.0020931389084938193)
760: accuracy:0.97 loss: 12.9771 (lr:0.002083198086715832)
770: accuracy:0.99 loss: 2.79813 (lr:0.002073306844993304)
780: accuracy:1.0 loss: 0.97761 (lr:0.0020634649360446776)
790: accuracy:0.97 loss: 7.81992 (lr:0.0020536721138217163)
800: accuracy:0.97 loss: 5.82343 (lr:0.002043928133503354)
800: ********* epoch 2 ********* test accuracy:0.9827 test loss: 5.62239
810: accuracy:0.95 loss: 10.3933 (lr:0.002034232751489576)
820: accuracy:0.98 loss: 5.71602 (lr:0.0020245857253953265)
830: accuracy:1.0 loss: 2.88497 (lr:0.00201498681404445)
840: accuracy:0.99 loss: 3.28415 (lr:0.0020054357774636645)
850: accuracy:0.98 loss: 8.60718 (lr:0.001995932376876557)
860: accuracy:0.98 loss: 2.84782 (lr:0.001986476374697618)
870: accuracy:0.99 loss: 3.1015 (lr:0.0019770675345263007)
880: accuracy:0.99 loss: 3.97043 (lr:0.00196770562114111)
890: accuracy:0.99 loss: 2.47886 (lr:0.0019583904004937245)
900: accuracy:0.99 loss: 2.73276 (lr:0.001949121639703143)
900: ********* epoch 2 ********* test accuracy:0.9846 test loss: 4.7996
910: accuracy:0.98 loss: 6.45912 (lr:0.001939899107049862)
920: accuracy:0.97 loss: 7.65443 (lr:0.0019307225719700854)
930: accuracy:0.98 loss: 7.98505 (lr:0.0019215918050499584)
940: accuracy:0.97 loss: 4.71026 (lr:0.0019125065780198325)
950: accuracy:0.98 loss: 7.83793 (lr:0.0019034666637485586)
960: accuracy:0.97 loss: 7.87269 (lr:0.0018944718362378086)
970: accuracy:0.98 loss: 3.49079 (lr:0.001885521870616427)
980: accuracy:0.96 loss: 14.8446 (lr:0.0018766165431348069)
990: accuracy:0.99 loss: 2.06713 (lr:0.001867755631159297)
1000: accuracy:0.99 loss: 1.85361 (lr:0.0018589389131666372)
1000: ********* epoch 2 ********* test accuracy:0.9868 test loss: 4.4302
1010: accuracy:0.96 loss: 10.748 (lr:0.0018501661687384176)
1020: accuracy:0.98 loss: 9.94855 (lr:0.0018414371785555712)
1030: accuracy:1.0 loss: 1.08613 (lr:0.0018327517243928889)
1040: accuracy:0.99 loss: 5.245 (lr:0.001824109589113564)
1050: accuracy:0.97 loss: 6.93233 (lr:0.001815510556663764)
1060: accuracy:0.99 loss: 3.87674 (lr:0.0018069544120672303)
1070: accuracy:0.98 loss: 9.8775 (lr:0.001798440941419902)
1080: accuracy:0.98 loss: 5.78042 (lr:0.0017899699318845701)
1090: accuracy:1.0 loss: 1.14611 (lr:0.0017815411716855546)
1100: accuracy:0.97 loss: 9.68068 (lr:0.0017731544501034114)
1100: ********* epoch 2 ********* test accuracy:0.9854 test loss: 4.56803
1110: accuracy:0.99 loss: 7.0469 (lr:0.0017648095574696644)
1120: accuracy:0.99 loss: 3.56217 (lr:0.0017565062851615633)
1130: accuracy:0.99 loss: 5.26773 (lr:0.0017482444255968678)
1140: accuracy:0.95 loss: 9.63642 (lr:0.0017400237722286578)
1150: accuracy:0.98 loss: 4.89157 (lr:0.0017318441195401718)
1160: accuracy:1.0 loss: 1.62719 (lr:0.001723705263039666)
1170: accuracy:0.99 loss: 4.11238 (lr:0.0017156069992553045)
1180: accuracy:1.0 loss: 0.538622 (lr:0.0017075491257300707)
1190: accuracy:0.98 loss: 5.3687 (lr:0.0016995314410167067)
1200: accuracy:0.99 loss: 5.60456 (lr:0.001691553744672677)
1200: ********* epoch 3 ********* test accuracy:0.9841 test loss: 4.66858
1210: accuracy:0.98 loss: 4.14555 (lr:0.0016836158372551574)
1220: accuracy:0.97 loss: 8.28485 (lr:0.0016757175203160495)
1230: accuracy:0.99 loss: 1.6056 (lr:0.0016678585963970183)
1240: accuracy:0.97 loss: 9.37022 (lr:0.001660038869024556)
1250: accuracy:0.99 loss: 8.02714 (lr:0.001652258142705072)
1260: accuracy:0.99 loss: 2.42853 (lr:0.001644516222920002)
1270: accuracy:1.0 loss: 1.28606 (lr:0.001636812916120949)
1280: accuracy:0.99 loss: 3.24205 (lr:0.001629148029724841)
1290: accuracy:0.99 loss: 2.13121 (lr:0.0016215213721091197)
1300: accuracy:0.98 loss: 5.02074 (lr:0.0016139327526069466)
1300: ********* epoch 3 ********* test accuracy:0.9853 test loss: 4.07297
1310: accuracy:0.98 loss: 7.34095 (lr:0.00160638198150244)
1320: accuracy:1.0 loss: 0.920606 (lr:0.0015988688700259279)
1330: accuracy:0.99 loss: 3.05999 (lr:0.0015913932303492327)
1340: accuracy:0.98 loss: 9.04136 (lr:0.0015839548755809732)
1350: accuracy:1.0 loss: 1.22469 (lr:0.0015765536197618927)
1360: accuracy:0.99 loss: 3.44275 (lr:0.0015691892778602098)
1370: accuracy:0.99 loss: 2.69277 (lr:0.0015618616657669942)
1380: accuracy:1.0 loss: 0.973671 (lr:0.0015545706002915614)
1390: accuracy:0.99 loss: 3.06329 (lr:0.0015473158991568946)
1400: accuracy:0.98 loss: 3.75277 (lr:0.0015400973809950877)
1400: ********* epoch 3 ********* test accuracy:0.9861 test loss: 3.99615
1410: accuracy:0.99 loss: 1.87733 (lr:0.001532914865342811)
1420: accuracy:0.98 loss: 4.46389 (lr:0.001525768172636799)
1430: accuracy:1.0 loss: 0.503799 (lr:0.0015186571242093616)
1440: accuracy:0.99 loss: 1.68621 (lr:0.001511581542283918)
1450: accuracy:0.99 loss: 2.36558 (lr:0.0015045412499705513)
1460: accuracy:1.0 loss: 0.299959 (lr:0.0014975360712615872)
1470: accuracy:0.98 loss: 7.99617 (lr:0.0014905658310271931)
1480: accuracy:0.99 loss: 1.78561 (lr:0.0014836303550109999)
1490: accuracy:1.0 loss: 0.969274 (lr:0.0014767294698257462)
1500: accuracy:1.0 loss: 0.239509 (lr:0.0014698630029489428)
1500: ********* epoch 3 ********* test accuracy:0.9867 test loss: 4.20318
1510: accuracy:1.0 loss: 1.27734 (lr:0.0014630307827185603)
1520: accuracy:1.0 loss: 0.795534 (lr:0.0014562326383287369)
1530: accuracy:0.98 loss: 2.44343 (lr:0.001449468399825509)
1540: accuracy:0.98 loss: 6.39883 (lr:0.0014427378981025616)
1550: accuracy:0.99 loss: 2.26157 (lr:0.001436040964897001)
1560: accuracy:0.99 loss: 2.34111 (lr:0.0014293774327851483)
1570: accuracy:0.98 loss: 4.18625 (lr:0.0014227471351783538)
1580: accuracy:1.0 loss: 0.750335 (lr:0.001416149906318832)
1590: accuracy:1.0 loss: 1.31001 (lr:0.0014095855812755176)
1600: accuracy:1.0 loss: 0.251938 (lr:0.0014030539959399427)
1600: ********* epoch 3 ********* test accuracy:0.9849 test loss: 4.39084
1610: accuracy:1.0 loss: 0.840776 (lr:0.0013965549870221337)
1620: accuracy:0.99 loss: 2.45106 (lr:0.0013900883920465294)
1630: accuracy:0.99 loss: 3.90874 (lr:0.0013836540493479186)
1640: accuracy:0.99 loss: 2.05775 (lr:0.001377251798067398)
1650: accuracy:0.98 loss: 7.18057 (lr:0.001370881478148353)
1660: accuracy:1.0 loss: 1.00825 (lr:0.0013645429303324535)
1670: accuracy:0.99 loss: 2.31012 (lr:0.0013582359961556737)
1680: accuracy:1.0 loss: 1.55392 (lr:0.0013519605179443314)
1690: accuracy:0.99 loss: 1.35297 (lr:0.0013457163388111437)
1700: accuracy:0.98 loss: 4.92124 (lr:0.0013395033026513076)
1700: ********* epoch 3 ********* test accuracy:0.9875 test loss: 3.57385
1710: accuracy:0.99 loss: 1.69071 (lr:0.001333321254138595)
1720: accuracy:1.0 loss: 1.07393 (lr:0.0013271700387214717)
1730: accuracy:1.0 loss: 0.807168 (lr:0.0013210495026192315)
1740: accuracy:0.98 loss: 11.0387 (lr:0.0013149594928181531)
1750: accuracy:1.0 loss: 0.759193 (lr:0.0013088998570676745)
1760: accuracy:0.98 loss: 3.02254 (lr:0.0013028704438765861)
1770: accuracy:0.97 loss: 9.06326 (lr:0.0012968711025092442)
1780: accuracy:0.98 loss: 4.04116 (lr:0.001290901682981802)
1790: accuracy:0.98 loss: 5.74431 (lr:0.0012849620360584606)
1800: accuracy:0.99 loss: 2.688 (lr:0.0012790520132477375)
1800: ********* epoch 4 ********* test accuracy:0.9867 test loss: 3.87357
1810: accuracy:0.98 loss: 4.39535 (lr:0.0012731714667987546)
1820: accuracy:1.0 loss: 0.930198 (lr:0.0012673202496975445)
1830: accuracy:1.0 loss: 0.448779 (lr:0.0012614982156633745)
1840: accuracy:1.0 loss: 1.03992 (lr:0.0012557052191450912)
1850: accuracy:0.99 loss: 1.64558 (lr:0.0012499411153174794)
1860: accuracy:0.99 loss: 1.57359 (lr:0.0012442057600776434)
1870: accuracy:0.99 loss: 2.3001 (lr:0.0012384990100414034)
1880: accuracy:0.98 loss: 2.23746 (lr:0.0012328207225397114)
1890: accuracy:0.98 loss: 8.3996 (lr:0.001227170755615084)
1900: accuracy:1.0 loss: 0.958063 (lr:0.0012215489680180538)
1900: ********* epoch 4 ********* test accuracy:0.9875 test loss: 4.14981
1910: accuracy:1.0 loss: 0.814758 (lr:0.001215955219203638)
1920: accuracy:1.0 loss: 0.530213 (lr:0.0012103893693278251)
1930: accuracy:0.99 loss: 3.06163 (lr:0.0012048512792440782)
1940: accuracy:1.0 loss: 0.481686 (lr:0.0011993408104998568)
1950: accuracy:0.99 loss: 4.06618 (lr:0.0011938578253331553)
1960: accuracy:1.0 loss: 0.146021 (lr:0.001188402186669059)
1970: accuracy:0.99 loss: 2.62382 (lr:0.0011829737581163168)
1980: accuracy:1.0 loss: 0.637794 (lr:0.0011775724039639326)
1990: accuracy:0.99 loss: 4.69087 (lr:0.0011721979891777712)
2000: accuracy:0.99 loss: 5.12561 (lr:0.0011668503793971828)
2000: ********* epoch 4 ********* test accuracy:0.9887 test loss: 3.75989
2010: accuracy:0.99 loss: 20.3419 (lr:0.0011615294409316448)
2020: accuracy:1.0 loss: 0.379629 (lr:0.0011562350407574179)
2030: accuracy:1.0 loss: 1.90084 (lr:0.0011509670465142223)
2040: accuracy:1.0 loss: 1.39057 (lr:0.0011457253265019273)
2050: accuracy:0.97 loss: 4.02408 (lr:0.0011405097496772598)
2060: accuracy:1.0 loss: 0.343893 (lr:0.0011353201856505275)
2070: accuracy:0.99 loss: 2.00707 (lr:0.0011301565046823595)
2080: accuracy:0.99 loss: 1.80111 (lr:0.0011250185776804627)
2090: accuracy:0.98 loss: 13.5633 (lr:0.0011199062761963943)
2100: accuracy:1.0 loss: 0.107765 (lr:0.0011148194724223506)
2100: ********* epoch 4 ********* test accuracy:0.9868 test loss: 3.99381
2110: accuracy:0.97 loss: 5.55451 (lr:0.0011097580391879731)
2120: accuracy:1.0 loss: 0.579906 (lr:0.0011047218499571666)
2130: accuracy:0.99 loss: 2.19713 (lr:0.0010997107788249386)
2140: accuracy:1.0 loss: 1.10194 (lr:0.0010947247005142493)
2150: accuracy:1.0 loss: 0.765221 (lr:0.001089763490372882)
2160: accuracy:0.99 loss: 2.41389 (lr:0.0010848270243703235)
2170: accuracy:0.99 loss: 1.32697 (lr:0.001079915179094668)
2180: accuracy:0.98 loss: 2.93179 (lr:0.0010750278317495268)
2190: accuracy:0.99 loss: 1.84429 (lr:0.0010701648601509621)
2200: accuracy:1.0 loss: 0.221627 (lr:0.0010653261427244307)
2200: ********* epoch 4 ********* test accuracy:0.9883 test loss: 3.48243
2210: accuracy:1.0 loss: 0.298125 (lr:0.001060511558501745)
2220: accuracy:0.99 loss: 1.67234 (lr:0.0010557209871180483)
2230: accuracy:0.98 loss: 3.56839 (lr:0.0010509543088088069)
2240: accuracy:0.99 loss: 1.54807 (lr:0.0010462114044068145)
2250: accuracy:1.0 loss: 0.400743 (lr:0.0010414921553392143)
2260: accuracy:1.0 loss: 1.13831 (lr:0.0010367964436245336)
2270: accuracy:1.0 loss: 1.13979 (lr:0.001032124151869735)
2280: accuracy:0.99 loss: 6.70003 (lr:0.0010274751632672816)
2290: accuracy:0.98 loss: 4.39026 (lr:0.0010228493615922153)
2300: accuracy:1.0 loss: 1.26726 (lr:0.0010182466311992545)
2300: ********* epoch 4 ********* test accuracy:0.9883 test loss: 3.66394
2310: accuracy:0.98 loss: 3.30397 (lr:0.0010136668570198987)
2320: accuracy:1.0 loss: 0.696403 (lr:0.0010091099245595554)
2330: accuracy:0.97 loss: 6.33018 (lr:0.0010045757198946755)
2340: accuracy:0.99 loss: 1.6639 (lr:0.0010000641296699067)
2350: accuracy:1.0 loss: 1.00165 (lr:0.0009955750410952577)
2360: accuracy:0.99 loss: 2.74408 (lr:0.0009911083419432806)
2370: accuracy:0.98 loss: 13.4132 (lr:0.000986663920546264)
2380: accuracy:1.0 loss: 0.825642 (lr:0.0009822416657934419)
2390: accuracy:0.99 loss: 2.49019 (lr:0.0009778414671282143)
2400: accuracy:1.0 loss: 1.13604 (lr:0.0009734632145453863)
2400: ********* epoch 5 ********* test accuracy:0.9889 test loss: 3.20063
2410: accuracy:1.0 loss: 0.0863874 (lr:0.0009691067985884144)
2420: accuracy:1.0 loss: 2.07795 (lr:0.0009647721103466736)
2430: accuracy:1.0 loss: 1.14977 (lr:0.0009604590414527313)
2440: accuracy:1.0 loss: 0.852927 (lr:0.0009561674840796413)
2450: accuracy:1.0 loss: 0.384137 (lr:0.0009518973309382452)
2460: accuracy:0.99 loss: 1.91095 (lr:0.0009476484752744924)
2470: accuracy:1.0 loss: 0.680502 (lr:0.0009434208108667696)
2480: accuracy:0.99 loss: 1.37055 (lr:0.0009392142320232469)
2490: accuracy:1.0 loss: 0.0649225 (lr:0.0009350286335792337)
2500: accuracy:0.99 loss: 1.47722 (lr:0.0009308639108945514)
2500: ********* epoch 5 ********* test accuracy:0.9884 test loss: 3.64773
2510: accuracy:0.99 loss: 1.73984 (lr:0.0009267199598509155)
2520: accuracy:1.0 loss: 0.55329 (lr:0.0009225966768493343)
2530: accuracy:0.99 loss: 1.09489 (lr:0.0009184939588075178)
2540: accuracy:0.99 loss: 2.68144 (lr:0.0009144117031573014)
2550: accuracy:1.0 loss: 0.866951 (lr:0.0009103498078420814)
2560: accuracy:0.99 loss: 5.55809 (lr:0.0009063081713142631)
2570: accuracy:0.99 loss: 2.16648 (lr:0.0009022866925327231)
2580: accuracy:1.0 loss: 0.105937 (lr:0.0008982852709602818)
2590: accuracy:1.0 loss: 0.566704 (lr:0.0008943038065611923)
2600: accuracy:1.0 loss: 0.628076 (lr:0.0008903421997986366)
2600: ********* epoch 5 ********* test accuracy:0.9889 test loss: 3.45158
2610: accuracy:1.0 loss: 0.499711 (lr:0.0008864003516322398)
2620: accuracy:1.0 loss: 0.26901 (lr:0.0008824781635155918)
2630: accuracy:1.0 loss: 0.452181 (lr:0.0008785755373937862)
2640: accuracy:1.0 loss: 0.36028 (lr:0.000874692375700966)
2650: accuracy:0.99 loss: 0.927267 (lr:0.0008708285813578872)
2660: accuracy:1.0 loss: 0.373492 (lr:0.0008669840577694896)
2670: accuracy:1.0 loss: 0.182193 (lr:0.0008631587088224834)
2680: accuracy:1.0 loss: 0.115161 (lr:0.0008593524388829455)
2690: accuracy:0.99 loss: 1.9143 (lr:0.0008555651527939294)
2700: accuracy:0.98 loss: 2.22478 (lr:0.0008517967558730855)
2700: ********* epoch 5 ********* test accuracy:0.9888 test loss: 3.30504
2710: accuracy:1.0 loss: 0.767938 (lr:0.0008480471539102946)
2720: accuracy:0.99 loss: 1.02223 (lr:0.0008443162531653122)
2730: accuracy:0.99 loss: 5.23615 (lr:0.0008406039603654254)
2740: accuracy:0.99 loss: 1.53485 (lr:0.0008369101827031209)
2750: accuracy:1.0 loss: 1.35003 (lr:0.0008332348278337648)
2760: accuracy:0.99 loss: 1.66161 (lr:0.000829577803873294)
2770: accuracy:1.0 loss: 0.964541 (lr:0.0008259390193959188)
2780: accuracy:1.0 loss: 0.186211 (lr:0.000822318383431838)
2790: accuracy:1.0 loss: 0.760957 (lr:0.0008187158054649635)
2800: accuracy:1.0 loss: 0.164752 (lr:0.0008151311954306589)
2800: ********* epoch 5 ********* test accuracy:0.99 test loss: 3.39422
2810: accuracy:1.0 loss: 1.15896 (lr:0.0008115644637134865)
2820: accuracy:0.99 loss: 1.71001 (lr:0.0008080155211449677)
2830: accuracy:0.99 loss: 1.4827 (lr:0.0008044842790013532)
2840: accuracy:1.0 loss: 0.113611 (lr:0.0008009706490014058)
2850: accuracy:0.98 loss: 5.43482 (lr:0.0007974745433041923)
2860: accuracy:1.0 loss: 0.0933036 (lr:0.0007939958745068883)
2870: accuracy:0.98 loss: 4.38892 (lr:0.0007905345556425924)
2880: accuracy:0.99 loss: 3.00474 (lr:0.0007870905001781532)
2890: accuracy:0.98 loss: 3.62094 (lr:0.0007836636220120043)
2900: accuracy:1.0 loss: 0.588616 (lr:0.0007802538354720133)
2900: ********* epoch 5 ********* test accuracy:0.9894 test loss: 3.36877
2910: accuracy:0.99 loss: 1.75896 (lr:0.000776861055313339)
2920: accuracy:0.99 loss: 2.85287 (lr:0.0007734851967163007)
2930: accuracy:1.0 loss: 0.404629 (lr:0.0007701261752842576)
2940: accuracy:1.0 loss: 0.349321 (lr:0.0007667839070414992)
2950: accuracy:1.0 loss: 0.1507 (lr:0.0007634583084311452)
2960: accuracy:0.99 loss: 1.07434 (lr:0.000760149296313057)
2970: accuracy:1.0 loss: 0.0738147 (lr:0.0007568567879617594)
2980: accuracy:1.0 loss: 0.185793 (lr:0.0007535807010643724)
2990: accuracy:0.99 loss: 19.3223 (lr:0.0007503209537185525)
3000: accuracy:1.0 loss: 0.281234 (lr:0.0007470774644304465)
3000: ********* epoch 6 ********* test accuracy:0.9904 test loss: 3.08205
3010: accuracy:0.99 loss: 2.33965 (lr:0.0007438501521126534)
3020: accuracy:1.0 loss: 1.03312 (lr:0.0007406389360821968)
3030: accuracy:1.0 loss: 0.799882 (lr:0.0007374437360585091)
3040: accuracy:0.99 loss: 2.52499 (lr:0.0007342644721614229)
3050: accuracy:1.0 loss: 0.273649 (lr:0.0007311010649091755)
3060: accuracy:0.99 loss: 1.1123 (lr:0.0007279534352164206)
3070: accuracy:1.0 loss: 1.21633 (lr:0.0007248215043922522)
3080: accuracy:1.0 loss: 0.0664845 (lr:0.0007217051941382361)
3090: accuracy:0.98 loss: 3.43641 (lr:0.0007186044265464543)
3100: accuracy:1.0 loss: 0.651012 (lr:0.0007155191240975549)
3100: ********* epoch 6 ********* test accuracy:0.9902 test loss: 3.48507
3110: accuracy:0.99 loss: 1.55333 (lr:0.0007124492096588165)
3120: accuracy:0.99 loss: 2.27292 (lr:0.0007093946064822178)
3130: accuracy:1.0 loss: 0.230894 (lr:0.0007063552382025206)
3140: accuracy:1.0 loss: 1.20341 (lr:0.0007033310288353594)
3150: accuracy:1.0 loss: 0.0125903 (lr:0.0007003219027753427)
3160: accuracy:0.99 loss: 1.21854 (lr:0.000697327784794162)
3170: accuracy:1.0 loss: 0.0509156 (lr:0.0006943486000387123)
3180: accuracy:1.0 loss: 0.305415 (lr:0.000691384274029219)
3190: accuracy:1.0 loss: 0.155025 (lr:0.0006884347326573779)
3200: accuracy:1.0 loss: 0.0455685 (lr:0.0006854999021845007)
3200: ********* epoch 6 ********* test accuracy:0.9897 test loss: 3.47515
3210: accuracy:1.0 loss: 0.638846 (lr:0.0006825797092396732)
3220: accuracy:0.99 loss: 10.21 (lr:0.0006796740808179191)
3230: accuracy:0.99 loss: 2.23222 (lr:0.000676782944278377)
3240: accuracy:1.0 loss: 0.0677793 (lr:0.0006739062273424826)
3250: accuracy:1.0 loss: 0.599943 (lr:0.0006710438580921628)
3260: accuracy:1.0 loss: 0.229191 (lr:0.0006681957649680373)
3270: accuracy:0.99 loss: 1.28812 (lr:0.0006653618767676294)
3280: accuracy:1.0 loss: 0.111574 (lr:0.0006625421226435866)
3290: accuracy:1.0 loss: 0.732933 (lr:0.0006597364321019091)
3300: accuracy:0.98 loss: 3.09007 (lr:0.000656944735000187)
3300: ********* epoch 6 ********* test accuracy:0.9881 test loss: 3.98475
3310: accuracy:0.99 loss: 1.70394 (lr:0.0006541669615458476)
3320: accuracy:1.0 loss: 0.856581 (lr:0.0006514030422944097)
3330: accuracy:1.0 loss: 0.632758 (lr:0.000648652908147748)
3340: accuracy:1.0 loss: 0.223216 (lr:0.0006459164903523658)
3350: accuracy:0.99 loss: 1.01264 (lr:0.0006431937204976754)
3360: accuracy:1.0 loss: 0.0506157 (lr:0.000640484530514289)
3370: accuracy:1.0 loss: 0.0743753 (lr:0.0006377888526723156)
3380: accuracy:0.99 loss: 2.95351 (lr:0.000635106619579669)
3390: accuracy:1.0 loss: 0.495222 (lr:0.0006324377641803819)
3400: accuracy:1.0 loss: 0.851564 (lr:0.0006297822197529307)
3400: ********* epoch 6 ********* test accuracy:0.9897 test loss: 3.42716
3410: accuracy:0.99 loss: 1.80683 (lr:0.0006271399199085659)
3420: accuracy:1.0 loss: 0.0358827 (lr:0.0006245107985896541)
3430: accuracy:1.0 loss: 0.203249 (lr:0.0006218947900680254)
3440: accuracy:1.0 loss: 0.258391 (lr:0.0006192918289433304)
3450: accuracy:1.0 loss: 0.0474989 (lr:0.0006167018501414055)
3460: accuracy:0.99 loss: 1.157 (lr:0.0006141247889126458)
3470: accuracy:1.0 loss: 0.0455117 (lr:0.0006115605808303861)
3480: accuracy:1.0 loss: 0.32248 (lr:0.000609009161789291)
3490: accuracy:1.0 loss: 0.0808757 (lr:0.0006064704680037515)
3500: accuracy:1.0 loss: 0.662175 (lr:0.000603944436006291)
3500: ********* epoch 6 ********* test accuracy:0.9899 test loss: 3.41362
3510: accuracy:1.0 loss: 0.277416 (lr:0.0006014310026459777)
3520: accuracy:1.0 loss: 0.12907 (lr:0.0005989301050868467)
3530: accuracy:0.99 loss: 1.7061 (lr:0.0005964416808063288)
3540: accuracy:1.0 loss: 0.64837 (lr:0.0005939656675936874)
3550: accuracy:1.0 loss: 0.297312 (lr:0.0005915020035484634)
3560: accuracy:1.0 loss: 0.273353 (lr:0.000589050627078927)
3570: accuracy:1.0 loss: 0.0367403 (lr:0.0005866114769005391)
3580: accuracy:1.0 loss: 0.107622 (lr:0.000584184492034418)
3590: accuracy:1.0 loss: 0.874814 (lr:0.000581769611805816)
3600: accuracy:1.0 loss: 0.130503 (lr:0.000579366775842601)
3600: ********* epoch 7 ********* test accuracy:0.9902 test loss: 3.41388
3610: accuracy:1.0 loss: 0.150096 (lr:0.0005769759240737492)
3620: accuracy:1.0 loss: 0.0950218 (lr:0.0005745969967278418)
3630: accuracy:1.0 loss: 0.342815 (lr:0.0005722299343315713)
3640: accuracy:1.0 loss: 0.144096 (lr:0.0005698746777082543)
3650: accuracy:1.0 loss: 0.0329966 (lr:0.0005675311679763527)
3660: accuracy:0.99 loss: 2.77173 (lr:0.000565199346548001)
3670: accuracy:1.0 loss: 0.105817 (lr:0.0005628791551275424)
3680: accuracy:1.0 loss: 0.337823 (lr:0.00056057053571007)
3690: accuracy:1.0 loss: 0.209284 (lr:0.0005582734305799787)
3700: accuracy:1.0 loss: 0.469479 (lr:0.0005559877823095201)
3700: ********* epoch 7 ********* test accuracy:0.9899 test loss: 3.38064
3710: accuracy:0.98 loss: 7.5752 (lr:0.0005537135337573688)
3720: accuracy:1.0 loss: 0.0606965 (lr:0.0005514506280671922)
3730: accuracy:1.0 loss: 0.523546 (lr:0.0005491990086662305)
3740: accuracy:0.99 loss: 1.49847 (lr:0.0005469586192638811)
3750: accuracy:0.99 loss: 1.33 (lr:0.0005447294038502926)
3760: accuracy:1.0 loss: 0.136962 (lr:0.0005425113066949633)
3770: accuracy:1.0 loss: 0.515371 (lr:0.0005403042723453488)
3780: accuracy:1.0 loss: 0.060007 (lr:0.0005381082456254755)
3790: accuracy:1.0 loss: 0.244396 (lr:0.0005359231716345609)
3800: accuracy:1.0 loss: 0.049399 (lr:0.0005337489957456418)
3800: ********* epoch 7 ********* test accuracy:0.9903 test loss: 3.24701
3810: accuracy:0.99 loss: 1.44867 (lr:0.0005315856636042072)
3820: accuracy:1.0 loss: 0.194756 (lr:0.0005294331211268412)
3830: accuracy:1.0 loss: 0.0370559 (lr:0.0005272913144998697)
3840: accuracy:0.99 loss: 12.2184 (lr:0.0005251601901780155)
3850: accuracy:1.0 loss: 0.191741 (lr:0.0005230396948830594)
3860: accuracy:1.0 loss: 0.797694 (lr:0.0005209297756025089)
3870: accuracy:0.99 loss: 3.68465 (lr:0.0005188303795882718)
3880: accuracy:1.0 loss: 0.302389 (lr:0.0005167414543553385)
3890: accuracy:0.99 loss: 1.32513 (lr:0.0005146629476804694)
3900: accuracy:1.0 loss: 0.100239 (lr:0.0005125948076008895)
3900: ********* epoch 7 ********* test accuracy:0.9902 test loss: 3.51296
3910: accuracy:0.99 loss: 1.52575 (lr:0.0005105369824129888)
3920: accuracy:1.0 loss: 0.0558225 (lr:0.0005084894206710306)
3930: accuracy:1.0 loss: 0.2146 (lr:0.0005064520711858646)
3940: accuracy:1.0 loss: 0.0736944 (lr:0.0005044248830236478)
3950: accuracy:1.0 loss: 0.0123346 (lr:0.0005024078055045702)
3960: accuracy:0.99 loss: 8.80997 (lr:0.0005004007882015892)
3970: accuracy:1.0 loss: 0.327424 (lr:0.0004984037809391673)
3980: accuracy:0.99 loss: 0.91213 (lr:0.0004964167337920192)
3990: accuracy:1.0 loss: 0.17777 (lr:0.0004944395970838627)
4000: accuracy:1.0 loss: 0.435951 (lr:0.0004924723213861769)
4000: ********* epoch 7 ********* test accuracy:0.9909 test loss: 3.43012
4010: accuracy:1.0 loss: 0.60032 (lr:0.000490514857516967)
4020: accuracy:1.0 loss: 0.290415 (lr:0.0004885671565395345)
4030: accuracy:1.0 loss: 0.549569 (lr:0.00048662916976125316)
4040: accuracy:1.0 loss: 0.0305395 (lr:0.00048470084873235297)
4050: accuracy:1.0 loss: 0.403177 (lr:0.00048278214524470766)
4060: accuracy:1.0 loss: 0.607854 (lr:0.00048087301133063005)
4070: accuracy:1.0 loss: 0.404079 (lr:0.0004789733992616726)
4080: accuracy:1.0 loss: 0.331981 (lr:0.00047708326154743513)
4090: accuracy:0.99 loss: 0.966934 (lr:0.00047520255093437617)
4100: accuracy:1.0 loss: 0.0157134 (lr:0.0004733312204046323)
4100: ********* epoch 7 ********* test accuracy:0.9902 test loss: 3.79188
4110: accuracy:1.0 loss: 0.232084 (lr:0.0004714692231748428)
4120: accuracy:1.0 loss: 0.427832 (lr:0.0004696165126949802)
4130: accuracy:0.98 loss: 2.68266 (lr:0.0004677730426471858)
4140: accuracy:1.0 loss: 0.518045 (lr:0.00046593876694461247)
4150: accuracy:1.0 loss: 0.587613 (lr:0.0004641136397302719)
4160: accuracy:0.99 loss: 1.4277 (lr:0.000462297615375889)
4170: accuracy:0.99 loss: 2.12506 (lr:0.0004604906484807602)
4180: accuracy:1.0 loss: 0.0626672 (lr:0.000458692693870619)
4190: accuracy:1.0 loss: 0.417152 (lr:0.0004569037065965064)
4200: accuracy:1.0 loss: 0.0394639 (lr:0.00045512364193364755)
4200: ********* epoch 8 ********* test accuracy:0.9905 test loss: 3.47698
4210: accuracy:1.0 loss: 0.0347721 (lr:0.00045335245538033307)
4220: accuracy:1.0 loss: 0.109611 (lr:0.0004515901026568069)
4230: accuracy:1.0 loss: 0.0936604 (lr:0.000449836539704159)
4240: accuracy:1.0 loss: 0.0631056 (lr:0.00044809172268322453)
4250: accuracy:1.0 loss: 0.0744649 (lr:0.00044635560797348693)
4260: accuracy:1.0 loss: 0.168095 (lr:0.00044462815217198804)
4270: accuracy:1.0 loss: 0.027371 (lr:0.0004429093120922428)
4280: accuracy:1.0 loss: 0.0237848 (lr:0.0004411990447631597)
4290: accuracy:0.99 loss: 1.34251 (lr:0.0004394973074279665)
4300: accuracy:0.99 loss: 0.963201 (lr:0.00043780405754314123)
4300: ********* epoch 8 ********* test accuracy:0.9902 test loss: 3.59038
4310: accuracy:1.0 loss: 0.0601258 (lr:0.00043611925277734854)
4320: accuracy:1.0 loss: 0.0158068 (lr:0.0004344428510103813)
4330: accuracy:1.0 loss: 0.614214 (lr:0.0004327748103321084)
4340: accuracy:1.0 loss: 0.399037 (lr:0.00043111508904142585)
4350: accuracy:1.0 loss: 0.0676176 (lr:0.00042946364564521496)
4360: accuracy:1.0 loss: 0.0339252 (lr:0.0004278204388573046)
4370: accuracy:1.0 loss: 0.0416018 (lr:0.0004261854275974398)
4380: accuracy:0.99 loss: 1.99236 (lr:0.0004245585709902538)
4390: accuracy:1.0 loss: 0.0696329 (lr:0.0004229398283642466)
4400: accuracy:1.0 loss: 0.140484 (lr:0.00042132915925076824)
4400: ********* epoch 8 ********* test accuracy:0.99 test loss: 3.61064
4410: accuracy:1.0 loss: 0.653589 (lr:0.00041972652338300717)
4420: accuracy:1.0 loss: 0.102528 (lr:0.0004181318806949831)
4430: accuracy:1.0 loss: 0.10192 (lr:0.0004165451913205458)
4440: accuracy:1.0 loss: 0.0618309 (lr:0.0004149664155923781)
4450: accuracy:1.0 loss: 0.0427058 (lr:0.00041339551404100485)
4460: accuracy:1.0 loss: 0.00965839 (lr:0.0004118324473938054)
4470: accuracy:0.99 loss: 1.62386 (lr:0.00041027717657403205)
4480: accuracy:1.0 loss: 0.164997 (lr:0.00040872966269983314)
4490: accuracy:1.0 loss: 0.0938281 (lr:0.00040718986708328155)
4500: accuracy:1.0 loss: 0.0883181 (lr:0.00040565775122940656)
4500: ********* epoch 8 ********* test accuracy:0.9905 test loss: 3.49092
4510: accuracy:1.0 loss: 0.835375 (lr:0.000404133276835232)
4520: accuracy:1.0 loss: 0.0840813 (lr:0.0004026164057888186)
4530: accuracy:1.0 loss: 0.00951275 (lr:0.0004011071001683111)
4540: accuracy:1.0 loss: 0.342058 (lr:0.0003996053222409906)
4550: accuracy:1.0 loss: 0.0761062 (lr:0.00039811103446233054)
4560: accuracy:1.0 loss: 0.212815 (lr:0.0003966241994750587)
4570: accuracy:1.0 loss: 0.444046 (lr:0.0003951447801082228)
4580: accuracy:1.0 loss: 0.0547872 (lr:0.00039367273937626184)
4590: accuracy:1.0 loss: 0.0679185 (lr:0.00039220804047808086)
4600: accuracy:1.0 loss: 0.0901796 (lr:0.0003907506467961309)
4600: ********* epoch 8 ********* test accuracy:0.9898 test loss: 3.64495
4610: accuracy:1.0 loss: 0.798582 (lr:0.00038930052189549403)
4620: accuracy:1.0 loss: 0.163461 (lr:0.0003878576295229724)
4630: accuracy:1.0 loss: 0.0410798 (lr:0.0003864219336061815)
4640: accuracy:1.0 loss: 0.10592 (lr:0.0003849933982526485)
4650: accuracy:1.0 loss: 0.318725 (lr:0.00038357198774891513)
4660: accuracy:1.0 loss: 0.189581 (lr:0.00038215766655964504)
4670: accuracy:1.0 loss: 0.0661535 (lr:0.0003807503993267346)
4680: accuracy:1.0 loss: 0.0594786 (lr:0.0003793501508684298)
4690: accuracy:1.0 loss: 0.0982254 (lr:0.0003779568861784461)
4700: accuracy:1.0 loss: 0.0887661 (lr:0.0003765705704250939)
4700: ********* epoch 8 ********* test accuracy:0.9898 test loss: 3.63733
4710: accuracy:1.0 loss: 0.0976674 (lr:0.00037519116895040703)
4720: accuracy:1.0 loss: 0.412916 (lr:0.0003738186472692768)
4730: accuracy:0.99 loss: 1.70124 (lr:0.00037245297106858964)
4740: accuracy:1.0 loss: 0.208126 (lr:0.0003710941062063696)
4750: accuracy:1.0 loss: 0.613083 (lr:0.00036974201871092414)
4760: accuracy:1.0 loss: 0.501508 (lr:0.00036839667477999555)
4770: accuracy:0.99 loss: 3.78183 (lr:0.0003670580407799155)
4780: accuracy:1.0 loss: 0.708476 (lr:0.00036572608324476403)
4790: accuracy:1.0 loss: 0.497339 (lr:0.0003644007688755338)
4800: accuracy:1.0 loss: 0.0436727 (lr:0.0003630820645392963)
4800: ********* epoch 9 ********* test accuracy:0.9905 test loss: 3.65675
4810: accuracy:1.0 loss: 0.310033 (lr:0.0003617699372683745)
4820: accuracy:1.0 loss: 0.11419 (lr:0.00036046435425951816)
4830: accuracy:1.0 loss: 0.233217 (lr:0.00035916528287308427)
4840: accuracy:1.0 loss: 0.0124818 (lr:0.00035787269063222043)
4850: accuracy:1.0 loss: 0.298405 (lr:0.0003565865452220532)
4860: accuracy:1.0 loss: 0.0368534 (lr:0.0003553068144888804)
4870: accuracy:1.0 loss: 0.0159205 (lr:0.00035403346643936713)
4880: accuracy:1.0 loss: 0.0106316 (lr:0.00035276646923974576)
4890: accuracy:1.0 loss: 0.0169247 (lr:0.0003515057912150203)
4900: accuracy:1.0 loss: 0.0853922 (lr:0.00035025140084817447)
4900: ********* epoch 9 ********* test accuracy:0.9902 test loss: 3.63109
4910: accuracy:1.0 loss: 0.0188325 (lr:0.0003490032667793838)
4920: accuracy:1.0 loss: 0.00654368 (lr:0.0003477613578052316)
4930: accuracy:1.0 loss: 0.676635 (lr:0.0003465256428779287)
4940: accuracy:1.0 loss: 0.0245957 (lr:0.00034529609110453763)
4950: accuracy:1.0 loss: 0.105296 (lr:0.00034407267174620007)
4960: accuracy:1.0 loss: 0.393829 (lr:0.0003428553542173683)
4970: accuracy:1.0 loss: 0.0177368 (lr:0.0003416441080850407)
4980: accuracy:1.0 loss: 0.143249 (lr:0.00034043890306800073)
4990: accuracy:1.0 loss: 0.0933572 (lr:0.00033923970903606046)
5001: accuracy:1.0 loss: 0.0601356 (lr:0.00033792750251215517)
5001: ********* epoch 9 ********* test accuracy:0.9907 test loss: 3.68357
max test accuracy: 0.9909

Process finished with exit code 0
